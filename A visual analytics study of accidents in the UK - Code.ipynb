{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. Introduction](#1.-Introduction)\n",
    "\n",
    "[2. Dataset Description](#2.-Dataset-Description)\n",
    "\n",
    "[3. Data processing](#3.-Data-processing)\n",
    "- [3.1 Missing data](#3.1-Missing-data)\n",
    "- [3.2 Outliers](#3.2-Outliers)\n",
    "- [3.3 Create features](#3.3-Create-features)\n",
    "\n",
    "[4. Analysis](#4.-Analysis)\n",
    "- [4.1 Temporal Analysis](#4.1-Temporal-Analysis)\n",
    "- [4.2 Spatial Analysis](#4.2-Spatial-Analysis)\n",
    "- [4.3 Temporal and Spatial Analysis](#4.3-Temporal-and-Spatial-Analysis)\n",
    "\n",
    "[5. Clustering](#5.Clustering)\n",
    "- [5.1 DBSCAN](#5.1-DBSCAN)\n",
    "- [5.2 K-DBSCAN](#5.2-K-DBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from datetime import datetime\n",
    "import random\n",
    "import altair as alt\n",
    "import geopandas as gpd\n",
    "import minisom\n",
    "from sklearn import preprocessing, cluster\n",
    "import scipy\n",
    "import calendar\n",
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from kneed import KneeLocator\n",
    "from ipyleaflet import Map, ImageOverlay, basemap_to_tiles, basemaps\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Import Sklearn DBSCAN algorithm\n",
    "from sklearn.cluster import DBSCAN as dbscan\n",
    "from sklearn.cluster import DBSCAN\n",
    "# Import Sklearn OPTICS algorithm\n",
    "from sklearn.cluster import OPTICS, cluster_optics_dbscan\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.dates as mdates\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from vega_datasets import data\n",
    "from scipy.cluster.vq import kmeans\n",
    "\n",
    "url = 'https://github.com/python-visualization/folium/blob/master/tests/us-counties.json'\n",
    "\n",
    "accidents_file = './input/Accident_Information.csv'\n",
    "lad_file = './input/Local_Authority_District_to_Country__December_2017__Lookup_in_the_United_Kingdom.csv'\n",
    "ward_file = './input/Ward_to_Local_Authority_District_to_County_to_Region_to_Country__December_2017__Lookup_in_United_Kingdom_version_2.csv'\n",
    "\n",
    "# Set parameters for number of min/max rows\n",
    "\n",
    "#pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "\n",
    "# Perceptually color space\n",
    "from colorspacious import cspace_convert\n",
    "import matplotlib.colors as col\n",
    "\n",
    "# Ticker\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Colormap\n",
    "import branca\n",
    "import branca.colormap as cm\n",
    "\n",
    "# Axes 3D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "# Multipoint\n",
    "from shapely.geometry import MultiPoint\n",
    "\n",
    "# KDBSCAN\n",
    "import kdbscan\n",
    "from kdbscan import KDBSCAN\n",
    "\n",
    "# define the number of kilometers in one radian\n",
    "kms_per_radian = 6371.0088\n",
    "spatial_dist_max = 20 / kms_per_radian\n",
    "temporal_dist_max = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean shift clustering\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import MeanShift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read shape files for geospatial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_region = pd.read_csv('input/uk_regions_data.csv')\n",
    "uk_region = uk_region.append({'Area_Code': 'E09000001', 'Region': 'London', 'Area': 'City of London'}, ignore_index=True)\n",
    "\n",
    "\n",
    "#gb = gpd.read_file('shapefiles/boundaries_gb.shp')\n",
    "gb = gpd.read_file('shapefiles/new/Local_Authority_Districts__May_2020__Boundaries_UK_BFE.shp')\n",
    "gb.rename(columns={'lad20cd': 'geo_code'},inplace=True)\n",
    "\n",
    "gb.crs = 'epsg:27700'\n",
    "gb.loc[gb['geo_code']=='E41000052','geo_code'] = 'E06000052'\n",
    "gb.loc[gb['geo_code']=='E41000324','geo_code'] = 'E09000033'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_date(df, column_name='start_date', color='#494949', title=''):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    ax = (df[column_name].groupby(df[column_name].dt.hour)\n",
    "                         .count()).plot(kind=\"bar\", color=color)\n",
    "    ax.set_facecolor('#eeeeee')\n",
    "    ax.set_xlabel(\"hour of the day\")\n",
    "    ax.set_ylabel(\"count\")\n",
    "    ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to handle spatial and spatio-temporal distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def great_circle2(lat1, long1, lat2, long2):\n",
    "\n",
    "    # Convert latitude and longitude to \n",
    "    # spherical coordinates in radians.\n",
    "    degrees_to_radians = math.pi/180.0\n",
    "        \n",
    "    # phi = 90 - latitude\n",
    "    phi1 = (90.0 - lat1)*degrees_to_radians\n",
    "    phi2 = (90.0 - lat2)*degrees_to_radians\n",
    "        \n",
    "    # theta = longitude\n",
    "    theta1 = long1*degrees_to_radians\n",
    "    theta2 = long2*degrees_to_radians\n",
    "    \n",
    "    # Compute spherical distance from spherical coordinates.\n",
    "        \n",
    "    # For two locations in spherical coordinates \n",
    "    # (1, theta, phi) and (1, theta, phi)\n",
    "    # cosine( arc length ) = \n",
    "    #    sin phi sin phi' cos(theta-theta') + cos phi cos phi'\n",
    "    # distance = rho * arc length\n",
    "    \n",
    "    cos = (math.sin(phi1)*math.sin(phi2)*math.cos(theta1 - theta2) + \n",
    "           math.cos(phi1)*math.cos(phi2))\n",
    "    \n",
    "    if (cos > 1.0):\n",
    "        cos = 1.0\n",
    "\n",
    "    arc = math.acos( cos )\n",
    "\n",
    "    # Remember to multiply arc by the radius of the earth \n",
    "    # in your favorite set of units to get length.\n",
    "    return arc\n",
    "\n",
    "def SpaceDistance(x,y):\n",
    "    try:\n",
    "        gc_dist = great_circle2(x[1],x[0],y[1],y[0])\n",
    "    except ValueError:\n",
    "        gc_dist = np.Infinity\n",
    "    \n",
    "    if (gc_dist>spatial_dist_max):\n",
    "        return np.Infinity\n",
    "    else:\n",
    "        return gc_dist\n",
    "    #return great_circle(x[1],x[0],y[1],y[0])\n",
    "\n",
    "def SpaceTimeDistance(x,y,diff_time,spatial_dist_max,temporal_dist_max):\n",
    "    #print('Params = {},{}'.format(spatial_dist_max,temporal_dist_max))\n",
    "    diff_time = math.fabs(x[2] - y[2])\n",
    "    if (np.isnan(diff_time) or diff_time > temporal_dist_max):\n",
    "        return np.Infinity\n",
    "    \n",
    "    try:\n",
    "        gc_dist = great_circle2(x[1],x[0],y[1],y[0])\n",
    "    except ValueError:\n",
    "        #print(x[1],x[0],y[1],y[0])\n",
    "        gc_dist = np.Infinity\n",
    "    \n",
    "    if (gc_dist>spatial_dist_max):\n",
    "        return np.Infinity\n",
    "    \n",
    "    ratio_t=diff_time/temporal_dist_max\n",
    "    ratio_d=gc_dist/spatial_dist_max\n",
    "    if (ratio_d>ratio_t):\n",
    "        return gc_dist\n",
    "    else:\n",
    "        return ratio_t * spatial_dist_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getColor (x, y, minX, maxX, minY, maxY):\n",
    "    wX=maxX-minX \n",
    "    wY=maxY-minY\n",
    "    rr=y-minY \n",
    "    cc=x-minX\n",
    "    #print(x,y)\n",
    "    if (wY < wX):   #scale vertically, i.e. modify rr\n",
    "        rr *= wX/wY  \n",
    "    else:           #scale horizontally, i.e. modify cc\n",
    "        cc *= wY/wX\n",
    "    maxD=max(wX,wY)\n",
    "    rr1=maxD-rr\n",
    "    cc1=maxD-cc\n",
    "    #print(rr,cc,maxD,rr1,cc1)\n",
    "    dc=[math.sqrt(rr*rr+cc*cc),math.sqrt(rr*rr+cc1*cc1),math.sqrt(rr1*rr1+cc*cc),math.sqrt(rr1*rr1+cc1*cc1)]\n",
    "    weights=[0.0,0.0,0.0,0.0]\n",
    "    for i in range(len(weights)):\n",
    "        weights[i]=(maxD-dc[i])/maxD\n",
    "        if (weights[i]<0):\n",
    "            weights[i]=0\n",
    "    #print(dc,weights)\n",
    "    reds=[228,25,255,37]\n",
    "    greens=[220,228,18,13]\n",
    "    blues=[0,218,6,252]\n",
    "    dr=0\n",
    "    dg=0\n",
    "    db=0\n",
    "    for i,weight in enumerate(weights):\n",
    "        dr += weight*reds[i]\n",
    "        dg += weight*greens[i]\n",
    "        db += weight*blues[i]\n",
    "    if (dr<0):\n",
    "        dr=0;\n",
    "    if (dr>255):\n",
    "        dr=255\n",
    "    if (dg<0):\n",
    "        dg=0;\n",
    "    if (dg>255):\n",
    "        dg=255        \n",
    "    if (db<0):\n",
    "        db=0;\n",
    "    if (db>255):\n",
    "        db=255  \n",
    "    #print(weights,dr,dg,db)\n",
    "    c_string = '#{:02x}{:02x}{:02x}'.format(int(dr),int(dg),int(db))    \n",
    "    return c_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df.Region=='London'][['Local_Authority_(District)','Area_Code']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_group_altair(df,year):\n",
    "    \n",
    "    df_alt = df[df.Year==year]\n",
    "    df_alt = df_alt.groupby(['Area_Code','Accident_Severity'])[['Accident_Index','Number_of_Casualties']].agg(\n",
    "                        {'Accident_Index':'count','Number_of_Casualties':'sum'})\n",
    "    df_alt = df_alt.unstack()\n",
    "    df_alt = df_alt.assign(Casualties = (df_alt[('Number_of_Casualties',   'Slight')] + \n",
    "                                           df_alt[('Number_of_Casualties',   'Serious')] + \n",
    "                                           df_alt[('Number_of_Casualties',   'Fatal')]))\n",
    "    df_alt.drop(columns=[('Number_of_Casualties',   'Slight'),\n",
    "                          ('Number_of_Casualties',   'Serious'),\n",
    "                          ('Number_of_Casualties',   'Fatal')],inplace=True,errors='ignore')\n",
    "    df_alt['Year'] = year\n",
    "    return df_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distplot_fig(data, x, hue=None, row=None, col=None, legend=True, hist=False, **kwargs):\n",
    "    \"\"\"A figure-level distribution plot with support for hue, col, row arguments.\"\"\"\n",
    "    bins = kwargs.pop('bins', None)\n",
    "    if (bins is None) and hist: \n",
    "        # Make sure that the groups have equal-sized bins\n",
    "        bins = np.histogram_bin_edges(data[x].dropna())\n",
    "    g = sns.FacetGrid(data, hue=hue, row=row, col=col)\n",
    "    g.map(sns.distplot, x, bins=bins, hist=hist, **kwargs)\n",
    "    if legend and (hue is not None) and (hue not in [x, row, col]):\n",
    "        g.add_legend(title=hue) \n",
    "    return g   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', 500)\n",
    "#pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(accidents_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lad = pd.read_csv(lad_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ward = pd.read_csv(ward_file)\n",
    "df_ward.drop_duplicates(subset=['LAD17NM','LAD17CD','CTRY17NM'],inplace=True)\n",
    "df_ward = df_ward[['LAD17NM','LAD17CD','CTRY17NM']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ward.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge accidents and LAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_lad[['LAD17NM','CTRY17NM','LAD17CD']], how='left', \n",
    "              left_on='Local_Authority_(District)', right_on='LAD17NM')\n",
    "\n",
    "df.drop(columns=['LAD17NM'],inplace=True,errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge accidents and Ward dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_ward, how='left', \n",
    "              left_on='Local_Authority_(Highway)', right_on='LAD17NM')\n",
    "\n",
    "df.drop(columns=['LAD17NM'],inplace=True,errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CTRY17NM_z'] = df['CTRY17NM_x'].where(df['CTRY17NM_x'].notnull(), df['CTRY17NM_y'])\n",
    "df['LAD17CD_z'] = df['LAD17CD_x'].where(df['LAD17CD_x'].notnull(), df['LAD17CD_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.CTRY17NM_z.isnull().groupby(df['Year']).sum().astype(int).reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.CTRY17NM_z.isna()==True]['Local_Authority_(District)'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manual imputation for Local Authority District"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CTRY17NM_z'] =  np.where(\n",
    "             df['Local_Authority_(District)']=='Edinburgh, City of', \n",
    "            'Scotland', \n",
    "             np.where(\n",
    "                df['Local_Authority_(District)']=='Rhondda, Cynon, Taff','Wales', \n",
    "             np.where(\n",
    "                df['Local_Authority_(District)']=='St. Albans','England', \n",
    "             np.where(\n",
    "                df['Local_Authority_(District)']=='Stratford-upon-Avon','England',  \n",
    "             np.where(\n",
    "                df['Local_Authority_(District)']=='St. Edmundsbury','England', \n",
    "             np.where(\n",
    "                df['Local_Authority_(District)']=='The Vale of Glamorgan','Wales', \n",
    "             np.where(\n",
    "                df['Local_Authority_(District)']=='London Airport (Heathrow)','England', \n",
    "             np.where(\n",
    "                df['Local_Authority_(District)']=='Western Isles','Scotland', df['CTRY17NM_z']     \n",
    "             ))))))))\n",
    "\n",
    "df['LAD17CD_z'] =  np.where(\n",
    "             df['Local_Authority_(District)']=='Edinburgh, City of', \n",
    "            'S12000036', \n",
    "             np.where(\n",
    "                df['Local_Authority_(District)']=='Rhondda, Cynon, Taff','W06000016', \n",
    "             np.where(\n",
    "                df['Local_Authority_(District)']=='St. Albans','E07000240', \n",
    "             np.where(\n",
    "                df['Local_Authority_(District)']=='Stratford-upon-Avon','E07000221',  \n",
    "             np.where(\n",
    "                df['Local_Authority_(District)']=='St. Edmundsbury','E07000204', \n",
    "             np.where(\n",
    "                df['Local_Authority_(District)']=='The Vale of Glamorgan','W06000014', \n",
    "             np.where(\n",
    "                df['Local_Authority_(District)']=='London Airport (Heathrow)','E09000017', \n",
    "             np.where(\n",
    "                df['Local_Authority_(District)']=='Western Isles','S12000013', df['LAD17CD_z']     \n",
    "             ))))))))\n",
    "\n",
    "df.drop(columns=['LAD17CD_x','LAD17CD_y'],inplace=True,errors='ignore')\n",
    "df.rename(columns={'LAD17CD_z': 'Area_Code'}, inplace=True)\n",
    "df['Area_Code'] = np.where(df['Local_Authority_(District)']=='St. Albans',\n",
    "                                               'E07000100',df['Area_Code'])\n",
    "df['Area_Code'] = np.where(df['Local_Authority_(District)']=='East Hertfordshire',\n",
    "                                               'E07000097',df['Area_Code'])\n",
    "df['Area_Code'] = np.where(df['Local_Authority_(District)']=='Welwyn Hatfield',\n",
    "                                               'E07000104',df['Area_Code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.CTRY17NM_z.isna()==True]['Local_Authority_(District)'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['CTRY17NM_x','CTRY17NM_y','InScotland'],inplace=True,errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'CTRY17NM_z': 'Country'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge accidents and Ward dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, uk_region[['Area_Code','Region']], how='left', \n",
    "              left_on='Area_Code', right_on='Area_Code')\n",
    "#df.drop(columns=['LAD17NM'],inplace=True,errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Date range',df.Date.min(),'until',df.Date.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_lad.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now remove some variables form the dataset since they will not be useful for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = df.isnull().sum(axis=0).reset_index()\n",
    "missing_df.columns = ['column_name','missing_count']\n",
    "missing_df = missing_df[missing_df['missing_count']>0]\n",
    "missing_df = missing_df.sort_values(by='missing_count')\n",
    "\n",
    "ind = np.arange(missing_df.shape[0])\n",
    "width = 0.5\n",
    "fig,ax = plt.subplots(figsize=(12,18))\n",
    "rects = ax.barh(ind,missing_df.missing_count.values,color='blue')\n",
    "ax.set_yticks(ind)\n",
    "ax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\n",
    "ax.set_xlabel(\"Count of missing values\")\n",
    "ax.set_title(\"Number of missing values in each column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col = ['Junction_Control', 'LSOA_of_Accident_Location', '2nd_Road_Class', '2nd_Road_Number',\n",
    "           'Pedestrian_Crossing-Human_Control', 'Pedestrian_Crossing-Physical_Facilities',\n",
    "            'Did_Police_Officer_Attend_Scene_of_Accident','1st_Road_Number'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=drop_col, axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing values - Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Time.isna()==True].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Time'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing values - Latitude/Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Latitude.isna()==True].head(2)\n",
    "df[df.Longitude.isna()==True].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Latitude','Longitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing values - Speed limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Speed_limit.isna()==True].head(2)\n",
    "df.dropna(subset=['Speed_limit'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_df = df.isnull().sum(axis=0).reset_index()\n",
    "missing_df.columns = ['column_name','missing_count']\n",
    "missing_df = missing_df[missing_df['missing_count']>0]\n",
    "missing_df = missing_df.sort_values(by='missing_count')\n",
    "\n",
    "ind = np.arange(missing_df.shape[0])\n",
    "width = 0.5\n",
    "fig,ax = plt.subplots(figsize=(12,18))\n",
    "rects = ax.barh(ind,missing_df.missing_count.values,color='blue')\n",
    "ax.set_yticks(ind)\n",
    "ax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\n",
    "ax.set_xlabel(\"Count of missing values\")\n",
    "ax.set_title(\"Number of missing values in each column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Region'] =  np.where(df.Region.isna()==True,\n",
    "             np.where(\n",
    "                df['Local_Authority_(Highway)']=='Gateshead','North East', \n",
    "             np.where(\n",
    "                df['Local_Authority_(Highway)']=='Northumberland','North East', \n",
    "             np.where(\n",
    "                df['Local_Authority_(Highway)']=='Hertfordshire','East', \n",
    "             np.where(\n",
    "                df['Local_Authority_(Highway)']=='Isles of Scilly','South West',  \n",
    "             np.where(\n",
    "                df['Local_Authority_(Highway)']=='City of London','London', \n",
    "             df['Region']     \n",
    "             ))))),df['Region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Create features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion of coordinates from string to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Start_Lat'] = pd.to_numeric(df[\"Start_Lat\"], downcast=\"float\")\n",
    "# df['Start_Lng'] = pd.to_numeric(df[\"Start_Lng\"], downcast=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion from string date to date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "df['Hour'] = df['Datetime'].dt.hour\n",
    "df['WeekYear'] = df['Datetime'].dt.isocalendar().week\n",
    "df['YearMonth'] = df['Datetime'].dt.to_period('M')\n",
    "df['DaysSince'] = np.where(df.Year>=2013,(df['Datetime'] - pd.to_datetime('2013-01-01')).dt.days,0)\n",
    "df['Month'] = df['Datetime'].dt.month.apply(lambda x: calendar.month_abbr[x])\n",
    "df['DayYear'] = df['Datetime'].dt.dayofyear\n",
    "\n",
    "df['Year_bin'] = np.where((df.Year >= 2006) & (df.Year <= 2008),'2006-2008',\n",
    "                     np.where(\n",
    "                        (df.Year >= 2009) & (df.Year <= 2011),'2009-2011', \n",
    "                     np.where(\n",
    "                        (df.Year >= 2012) & (df.Year <= 2014),'2012-2014', \n",
    "                     np.where(\n",
    "                         (df.Year >= 2015) & (df.Year <= 2017),'2015-2017',df.Year\n",
    "                     ))))\n",
    "\n",
    "df['Hour_bin'] = np.where((df.Hour >= 0) & (df.Hour < 4),'0-4h',\n",
    "                     np.where(\n",
    "                        (df.Hour >= 4) & (df.Hour < 8),'4-8h', \n",
    "                     np.where(\n",
    "                        (df.Hour >= 8) & (df.Hour < 12),'8-12h', \n",
    "                     np.where(\n",
    "                         (df.Hour >= 12) & (df.Hour < 16),'12-16h',\n",
    "                     np.where(\n",
    "                        (df.Hour >= 16) & (df.Hour < 20),'16-20h', \n",
    "                     np.where(\n",
    "                        (df.Hour >= 20) & (df.Hour < 24),'20-24h', df.Hour\n",
    "                     ))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.hist(df['Accident_Severity'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accidents per LA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top15_lad = df[df['Local_Authority_(District)'].isin(df['Local_Authority_(District)'].value_counts()[:15].index)]\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "chart = sns.countplot(x=df_top15_lad['Local_Authority_(District)'],data=df_top15_lad,order = df_top15_lad['Local_Authority_(District)'].value_counts().index)\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accidents per LA and Year bin (5 groups of 3 years each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1la = df[df.Year_bin=='2006-2008']\n",
    "df2la = df[df.Year_bin=='2009-2011']\n",
    "df3la = df[df.Year_bin=='2012-2014']\n",
    "df4la = df[df.Year_bin=='2015-2017']\n",
    "\n",
    "df1_top15 = df1la[df1la['Local_Authority_(District)'].isin(df1la['Local_Authority_(District)'].value_counts()[:15].index)]\n",
    "df2_top15 = df2la[df2la['Local_Authority_(District)'].isin(df2la['Local_Authority_(District)'].value_counts()[:15].index)]\n",
    "df3_top15 = df3la[df3la['Local_Authority_(District)'].isin(df3la['Local_Authority_(District)'].value_counts()[:15].index)]\n",
    "df4_top15 = df4la[df4la['Local_Authority_(District)'].isin(df4la['Local_Authority_(District)'].value_counts()[:15].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(2,2,figsize=(16,8))\n",
    "\n",
    "sns.countplot(x=df1_top15['Local_Authority_(District)'],data=df1_top15,\n",
    "              order = df1_top15['Local_Authority_(District)'].value_counts().index,ax=ax[0][0])\n",
    "sns.countplot(x=df2_top15['Local_Authority_(District)'],\n",
    "                      data=df2_top15,\n",
    "                      order = df2_top15['Local_Authority_(District)'].value_counts().index,\n",
    "                      ax=ax[0][1])\n",
    "sns.countplot(x=df3_top15['Local_Authority_(District)'],\n",
    "                      data=df3_top15,\n",
    "                      order = df3_top15['Local_Authority_(District)'].value_counts().index,\n",
    "                      ax=ax[1][0])\n",
    "sns.countplot(x=df4_top15['Local_Authority_(District)'],\n",
    "                      data=df4_top15,\n",
    "                      order = df4_top15['Local_Authority_(District)'].value_counts().index,\n",
    "                      ax=ax[1][1])\n",
    "\n",
    "ax[0][0].set_xticklabels(ax[0][0].get_xticklabels(), rotation=90)\n",
    "ax[0][1].set_xticklabels(ax[0][1].get_xticklabels(), rotation=90)\n",
    "ax[1][0].set_xticklabels(ax[1][0].get_xticklabels(), rotation=90)\n",
    "ax[1][1].set_xticklabels(ax[1][1].get_xticklabels(), rotation=90)\n",
    "\n",
    "ax[0][0].set_title('Accidents per LA (2006-2008)\\n')\n",
    "ax[0][1].set_title('Accidents per LA (2009-2011)\\n')\n",
    "ax[1][0].set_title('Accidents per LA (2012-2014)\\n')\n",
    "ax[1][1].set_title('Accidents per LA (2015-2017)\\n')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accidents per Region and Year bin (5 groups of 3 years each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1_top15 = df1la[df1la['Region'].isin(df1la['Region'].value_counts()[:15].index)]\n",
    "df2_top15 = df2la[df2la['Region'].isin(df2la['Region'].value_counts()[:15].index)]\n",
    "df3_top15 = df3la[df3la['Region'].isin(df3la['Region'].value_counts()[:15].index)]\n",
    "df4_top15 = df4la[df4la['Region'].isin(df4la['Region'].value_counts()[:15].index)]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2,2,figsize=(16,12),sharey=True)\n",
    "\n",
    "sns.countplot(x=df1_top15['Region'],data=df1_top15,\n",
    "              order = df1_top15['Region'].value_counts().index,ax=ax[0][0])\n",
    "sns.countplot(x=df2_top15['Region'],\n",
    "                      data=df2_top15,\n",
    "                      order = df2_top15['Region'].value_counts().index,\n",
    "                      ax=ax[0][1])\n",
    "sns.countplot(x=df3_top15['Region'],\n",
    "                      data=df3_top15,\n",
    "                      order = df3_top15['Region'].value_counts().index,\n",
    "                      ax=ax[1][0])\n",
    "sns.countplot(x=df4_top15['Region'],\n",
    "                      data=df4_top15,\n",
    "                      order = df4_top15['Region'].value_counts().index,\n",
    "                      ax=ax[1][1])\n",
    "\n",
    "ax[0][0].set_xticklabels(ax[0][0].get_xticklabels(), rotation=90)\n",
    "ax[0][1].set_xticklabels(ax[0][1].get_xticklabels(), rotation=90)\n",
    "ax[1][0].set_xticklabels(ax[1][0].get_xticklabels(), rotation=90)\n",
    "ax[1][1].set_xticklabels(ax[1][1].get_xticklabels(), rotation=90)\n",
    "\n",
    "ax[0][0].set_title('Accidents per Region (2006-2008)\\n')\n",
    "ax[0][1].set_title('Accidents per Region (2009-2011)\\n')\n",
    "ax[1][0].set_title('Accidents per Region (2012-2014)\\n')\n",
    "ax[1][1].set_title('Accidents per Region (2015-2017)\\n')\n",
    "\n",
    "ax[0][0].set_xlabel('')\n",
    "ax[0][1].set_xlabel('')\n",
    "ax[1][0].set_xlabel('')\n",
    "ax[1][1].set_xlabel('')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number_of_Casualties (Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "sns.countplot(df.Number_of_Casualties)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number_of_Casualties per Year bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All casualties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sns.countplot(df.Number_of_Casualties)\n",
    "\n",
    "grid = sns.FacetGrid(df[df.Year>2005], col='Year_bin',height=4,aspect=1.1)\n",
    "\n",
    "grid.map(sns.countplot, 'Number_of_Casualties')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Only multiple casualties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sns.countplot(df.Number_of_Casualties)\n",
    "\n",
    "grid = sns.FacetGrid(df[df.Number_of_Casualties > 1 & (df.Year>2005)], col='Year_bin', height=7, aspect=0.8)\n",
    "\n",
    "grid.map(sns.countplot, 'Number_of_Casualties')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Number_of_Casualties.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Number_of_Casualties.isin([87,93])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "93 Casualties on 2014-10-20 in Hertfordshire:\n",
    "* The accident happened in a double decker bus and despite the severity of the accident it was not fatal.    \n",
    "* Reference: https://www.bbc.co.uk/news/uk-england-beds-bucks-herts-35416869\n",
    "    \n",
    "87 Casualties on 2014-06-03 in County Durham:\n",
    "* The accident happened between two buses of children and it was not fatal. The reported casualties number was supposedly lower than the one from the dataset at around 50 casualties.\n",
    "* Reference: https://www.chroniclelive.co.uk/news/north-east-news/stanley-bus-crash-view-images-7207590\n",
    "\n",
    "87 Casualties on 2011-06-20\tin Salford:\n",
    "* The accident happened between two buses of children and it was not fatal. The reported casualties number was supposedly lower than the one from the dataset at around 50 casualties.\n",
    "* Reference: https://www.lancashiretelegraph.co.uk/news/9095178.north-west-motorway-tailbacks-school-coach-crash/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number_of_Vehicles (Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "sns.countplot(df.Number_of_Vehicles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Number_of_Vehicles > 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "67 vehicles accidents with 70 casualties on 2013-09-05 in Kent:\n",
    "Reference: https://www.theguardian.com/uk-news/2013/sep/05/car-pileup-sheppey-bridge-kent\n",
    "    \n",
    "37 vehicles accidents with 36 casualties on 2015-02-14 in Kent:\n",
    "Reference:\n",
    "    \n",
    "34 vehicles accidents with 51 casualties on 2011-11-04 in Kent:\n",
    "Reference:\n",
    "    \n",
    "32 vehicles accidents with 5 casualties on 2009-12-23 in Kent:\n",
    "Reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df[df.Year>=2010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df[df.Year>=2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a.groupby(['Year','Day_of_Week'])['Accident_Index'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Year>=2015].groupby(['Date'])['Accident_Index'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df[(df.Year>=2010) & (df.Accident_Severity.isin(['Fatal','Serious']))].groupby(['Year','Month'])['Accident_Index'].count().reset_index()\n",
    "\n",
    "df2010 = a[a.Year==2010]\n",
    "df2011 = a[a.Year==2011]\n",
    "df2012 = a[a.Year==2012]\n",
    "df2013 = a[a.Year==2013]\n",
    "df2014 = a[a.Year==2014]\n",
    "df2015 = a[a.Year==2015]\n",
    "df2016 = a[a.Year==2016]\n",
    "df2017 = a[a.Year==2017]\n",
    "\n",
    "dfall = a.groupby(['Month'])['Accident_Index'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2010.plot.bar('Month','Accident_Index')\n",
    "df2011.plot.bar(x='Month',y='Accident_Index')\n",
    "df2012.plot.bar(x='Month',y='Accident_Index')\n",
    "df2013.plot.bar(x='Month',y='Accident_Index')\n",
    "df2014.plot.bar(x='Month',y='Accident_Index')\n",
    "df2015.plot.bar(x='Month',y='Accident_Index')\n",
    "df2016.plot.bar(x='Month',y='Accident_Index')\n",
    "df2017.plot.bar(x='Month',y='Accident_Index')\n",
    "\n",
    "dfall.plot.bar(x='Month',y='Accident_Index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accidents per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df['Year'])['Accident_Index'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date = df.groupby(df.Date)['Accident_Index'].count().reset_index()\n",
    "df_date = df_date.set_index('Date')\n",
    "#df [(df.index > '2016-02-08') & (df.index < '2016-03-08')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairplot = df[(df.Date>='2005-01-01') & (df.Date<='2005-02-01')]\n",
    "df_pairplot = df_pairplot.groupby([df_pairplot.Date, df_pairplot.Accident_Severity,\n",
    "                                  df_pairplot.Hour, df_pairplot.DaysSince,\n",
    "                                  df_pairplot.Year])['Accident_Index','Number_of_Casualties'].agg(\n",
    "                                 {'Accident_Index':'count', 'Number_of_Casualties':'sum'}).reset_index()\n",
    "df_pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1la[df1la.Region=='West Midlands']['Accident_Index'].count())\n",
    "print(df1la[df1la.Region=='East']['Accident_Index'].count())\n",
    "print(df1la[df1la.Region=='North West']['Accident_Index'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pairplot = df[(df.Date>='2005-01-01')]\n",
    "# df_pairplot = df_pairplot.groupby([df_pairplot.Date, df_pairplot.Accident_Severity,\n",
    "#                                   df_pairplot.Hour, df_pairplot.DaysSince,\n",
    "#                                   df_pairplot.Year])['Accident_Index','Number_of_Casualties','Latitude'].agg(\n",
    "#                                  {'Accident_Index':'count', 'Number_of_Casualties':'sum',\n",
    "#                                  'Latitude':'mean'}).reset_index()\n",
    "# df_pairplot.rename(columns={'Accident_Index': 'Number_of_Daily_Accidents'}, errors='ignore', inplace=True)\n",
    "\n",
    "\n",
    "# lm = sns.pairplot(df_pairplot[['Number_of_Daily_Accidents','Number_of_Casualties','Hour',\n",
    "#                           'Year','Latitude','Accident_Severity']], hue='Accident_Severity', \n",
    "#                           diag_kind='kde', plot_kws=dict(alpha=1.0))\n",
    "\n",
    "# axes = lm.axes\n",
    "\n",
    "# axes[3,0].set_ylim(2004,2018)\n",
    "# axes[4,3].set_xlim(2004,2018)\n",
    "\n",
    "\n",
    "# for ax in lm.axes[-1,:]:\n",
    "#     xlabel = ax.xaxis.get_label_text()\n",
    "#     ax.xaxis.set_label_text(xlabel,fontsize=12)\n",
    "# for ax in lm.axes[:,0]:\n",
    "#     ylabel = ax.yaxis.get_label_text()\n",
    "#     ax.yaxis.set_label_text(ylabel,fontsize=12)\n",
    "\n",
    "# plt.setp(lm._legend.get_title(), fontsize=12)\n",
    "\n",
    "# plt.setp(lm._legend.get_texts(), fontsize=12)\n",
    "\n",
    "# lm.savefig('pairplot.png')\n",
    "\n",
    "\n",
    "\n",
    "df1_top15 = df1la[df1la['Region'].isin(df1la['Region'].value_counts()[:15].index)]\n",
    "df2_top15 = df2la[df2la['Region'].isin(df2la['Region'].value_counts()[:15].index)]\n",
    "df3_top15 = df3la[df3la['Region'].isin(df3la['Region'].value_counts()[:15].index)]\n",
    "df4_top15 = df4la[df4la['Region'].isin(df4la['Region'].value_counts()[:15].index)]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2,2,figsize=(16,12),sharey=True)\n",
    "\n",
    "\n",
    "\n",
    "sns.countplot(x=df1_top15['Region'],data=df1_top15,\n",
    "              order = df1_top15['Region'].value_counts().index,ax=ax[0][0])\n",
    "sns.countplot(x=df2_top15['Region'],\n",
    "                      data=df2_top15,\n",
    "                      order = df2_top15['Region'].value_counts().index,\n",
    "                      ax=ax[0][1])\n",
    "sns.countplot(x=df3_top15['Region'],\n",
    "                      data=df3_top15,\n",
    "                      order = df3_top15['Region'].value_counts().index,\n",
    "                      ax=ax[1][0])\n",
    "sns.countplot(x=df4_top15['Region'],\n",
    "                      data=df4_top15,\n",
    "                      order = df4_top15['Region'].value_counts().index,\n",
    "                      ax=ax[1][1])\n",
    "\n",
    "\n",
    "\n",
    "ax[0][0].yaxis.set_tick_params(labelsize=12)\n",
    "ax[1][0].yaxis.set_tick_params(labelsize=12)\n",
    "\n",
    "ax[0][0].set_xticklabels(ax[0][0].get_xticklabels(), rotation=90, fontsize=14)\n",
    "ax[0][1].set_xticklabels(ax[0][1].get_xticklabels(), rotation=90, fontsize=14)\n",
    "ax[1][0].set_xticklabels(ax[1][0].get_xticklabels(), rotation=90, fontsize=14)\n",
    "ax[1][1].set_xticklabels(ax[1][1].get_xticklabels(), rotation=90, fontsize=14)\n",
    "\n",
    "#ax[0][0].set_yticklabels(ax[0][0].get_yticklabels(), fontsize=14)\n",
    "#ax[0][1].set_yticklabels(ax[0][1].get_yticklabels(), fontsize=14)\n",
    "#ax[0][1].set_yticklabels(fontsize=14)\n",
    "#ax[1][0].set_yticklabels(fontsize=14)\n",
    "#ax[1][1].set_yticklabels(fontsize=14)\n",
    "\n",
    "\n",
    "\n",
    "ax[0][0].set_title('Accidents per Region (2006-2008)\\n', fontsize=14)\n",
    "ax[0][1].set_title('Accidents per Region (2009-2011)\\n', fontsize=14)\n",
    "ax[1][0].set_title('Accidents per Region (2012-2014)\\n', fontsize=14)\n",
    "ax[1][1].set_title('Accidents per Region (2015-2017)\\n', fontsize=14)\n",
    "\n",
    "ax[0][0].set_xlabel('')\n",
    "ax[0][1].set_xlabel('')\n",
    "ax[1][0].set_xlabel('')\n",
    "ax[1][1].set_xlabel('')\n",
    "\n",
    "ax[0][0].set_ylabel('')\n",
    "ax[0][1].set_ylabel('')\n",
    "ax[1][0].set_ylabel('')\n",
    "ax[1][1].set_ylabel('')\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig.savefig('region.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_2005 = df_date[(df_date.index=='2005-01-01')]['Accident_Index'].values\n",
    "line_2006 = df_date[(df_date.index=='2006-01-01')]['Accident_Index'].values\n",
    "line_2007 = df_date[(df_date.index=='2007-01-01')]['Accident_Index'].values\n",
    "line_2008 = df_date[(df_date.index=='2008-01-01')]['Accident_Index'].values\n",
    "line_2009 = df_date[(df_date.index=='2009-01-01')]['Accident_Index'].values\n",
    "line_2010 = df_date[(df_date.index=='2010-01-01')]['Accident_Index'].values\n",
    "line_2011 = df_date[(df_date.index=='2011-01-01')]['Accident_Index'].values\n",
    "line_2012 = df_date[(df_date.index=='2012-01-01')]['Accident_Index'].values\n",
    "line_2013 = df_date[(df_date.index=='2013-01-01')]['Accident_Index'].values\n",
    "line_2014 = df_date[(df_date.index=='2014-01-01')]['Accident_Index'].values\n",
    "line_2015 = df_date[(df_date.index=='2015-01-01')]['Accident_Index'].values\n",
    "line_2016 = df_date[(df_date.index=='2016-01-01')]['Accident_Index'].values\n",
    "line_2017 = df_date[(df_date.index=='2017-01-01')]['Accident_Index'].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Converting the index as date\n",
    "df_date.index = pd.to_datetime(df_date.index)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(14,6))\n",
    "\n",
    "ax = sns.lineplot(x=df_date.index, y='Accident_Index', data=df_date)\n",
    "\n",
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "years_fmt = mdates.DateFormatter('%Y')\n",
    "\n",
    "# format the ticks\n",
    "ax.xaxis.set_major_locator(years)\n",
    "ax.xaxis.set_major_formatter(years_fmt)\n",
    "ax.xaxis.set_minor_locator(months)\n",
    "\n",
    "# format the coords message box\n",
    "ax.format_xdata = mdates.DateFormatter('%Y-%m-%d')\n",
    "#ax.grid(True)\n",
    "\n",
    "# round to nearest years.\n",
    "datemin = np.datetime64(str(df_date.index[0]), 'Y')\n",
    "datemax = np.datetime64(str(df_date.index[-1]), 'Y') + np.timedelta64(1, 'Y')\n",
    "ax.set_xlim(datemin, '2011-01-01')\n",
    "\n",
    "ax.tick_params(which=\"both\", bottom=True,length=6, width=1)\n",
    "plt.axvline('2006-01-01', color='red', linestyle='dashed')\n",
    "# plt.axvline(line_2006, color='red', linestyle='dashed')\n",
    "# plt.axvline(line_2007, color='red', linestyle='dashed')\n",
    "# plt.axvline(line_2008, color='red', linestyle='dashed')\n",
    "# plt.axvline(line_2009, color='red', linestyle='dashed')\n",
    "# plt.axvline(line_2010, color='red', linestyle='dashed')\n",
    "# plt.axvline(line_2011, color='red', linestyle='dashed')\n",
    "# plt.axvline(line_2012, color='red', linestyle='dashed')\n",
    "# plt.axvline(line_2013, color='red', linestyle='dashed')\n",
    "# plt.axvline(line_2014, color='red', linestyle='dashed')\n",
    "# plt.axvline(line_2015, color='red', linestyle='dashed')\n",
    "# plt.axvline(line_2016, color='red', linestyle='dashed')\n",
    "# plt.axvline(line_2017, color='red', linestyle='dashed')\n",
    "\n",
    "# rotates and right aligns the x labels, and moves the bottom of the\n",
    "# axes up to make room for them\n",
    "fig.autofmt_xdate()\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#fig.savefig('all_accidents.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accidents per severity per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_sev = df.groupby([df.Date,'Accident_Severity']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_sev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "\n",
    "# Converting the index as date\n",
    "df_date_sev.index = pd.to_datetime(df_date_sev.Date)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(12,4))\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(14,6))\n",
    "ax = sns.lineplot(x=df_date_sev[df_date_sev.index<'2007-01-01'].index, y='Accident_Index', hue='Accident_Severity',\n",
    "             data=df_date_sev[df_date_sev.index<'2007-01-01'],palette=sns.color_palette('bright', df_date_sev.Accident_Severity.unique().shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "# format the coords message box\n",
    "ax.format_xdata = mdates.DateFormatter('%Y-%m-%d')\n",
    "# ax.grid(True)\n",
    "\n",
    "# round to nearest years.\n",
    "#datemin = np.datetime64(str(df_date_sev.index[0]), 'Y')\n",
    "#datemax = np.datetime64(str(df_date_sev.index[-1]), 'Y') + np.timedelta64(1, 'Y')\n",
    "#ax.set_xlim(datemin, datemax)\n",
    "\n",
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "years_fmt = mdates.DateFormatter('%Y')\n",
    "# format the ticks\n",
    "ax.xaxis.set_major_locator(years)\n",
    "ax.xaxis.set_major_formatter(years_fmt)\n",
    "ax.xaxis.set_minor_locator(months)\n",
    "ax.minorticks_on()\n",
    "\n",
    "ax.tick_params(which=\"both\", bottom=True,length=6, width=1)\n",
    "\n",
    "#start, end = plt.xlim()\n",
    "#plt.xticks(np.arange(start, end, (start+end)/10))\n",
    "\n",
    "# plt.axvline('2005-01-01', color='black', linestyle='dashed')\n",
    "# plt.axvline('2006-01-01', color='black', linestyle='dashed')\n",
    "# plt.axvline('2007-01-01', color='black', linestyle='dashed')\n",
    "# plt.axvline('2008-01-01', color='black', linestyle='dashed')\n",
    "# plt.axvline('2009-01-01', color='black', linestyle='dashed')\n",
    "# plt.axvline('2010-01-01', color='black', linestyle='dashed')\n",
    "# plt.axvline('2011-01-01', color='black', linestyle='dashed')\n",
    "# plt.axvline('2012-01-01', color='black', linestyle='dashed')\n",
    "# plt.axvline('2013-01-01', color='black', linestyle='dashed')\n",
    "# plt.axvline('2014-01-01', color='black', linestyle='dashed')\n",
    "# plt.axvline('2015-01-01', color='black', linestyle='dashed')\n",
    "# plt.axvline('2016-01-01', color='black', linestyle='dashed')\n",
    "# plt.axvline('2017-01-01', color='black', linestyle='dashed')\n",
    "\n",
    "#fig.autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Only Serious accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "sns.lineplot(x=df_date_sev[df_date_sev.Accident_Severity=='Serious'].Date, y='Accident_Index',\n",
    "             data=df_date_sev[df_date_sev.Accident_Severity=='Serious'])\n",
    "start, end = plt.xlim()\n",
    "plt.xticks(np.arange(start, end, (start+end)/10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Only fatal accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_sev[df_date_sev.Accident_Severity=='Fatal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = df_date_sev\n",
    "df11.groupby(df11.index.year)['Accident_Index'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "sns.lineplot(x=df_date_sev[df_date_sev.Accident_Severity=='Fatal'].Date, y='Accident_Index',\n",
    "             data=df_date_sev[df_date_sev.Accident_Severity=='Fatal'])\n",
    "start, end = plt.xlim()\n",
    "plt.xticks(np.arange(start, end, (start+end)/10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serious/Fatal accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(24,20))\n",
    "\n",
    "# Converting the index as date\n",
    "#df_date_sev.index = pd.to_datetime(df_date_sev.Date)\n",
    "df_date_sev2 = df_date_sev[df_date_sev.Accident_Severity.isin(['Serious','Fatal'])]\n",
    "\n",
    "ax1 = plt.subplot(211)\n",
    "sns.lineplot(x=df_date_sev.index, y='Accident_Index', hue='Accident_Severity',\n",
    "             data=df_date_sev,palette=sns.color_palette('bright', df_date_sev.Accident_Severity.unique().shape[0]),ax=ax1)\n",
    "# start, end = ax1.set_xlim()\n",
    "# ax1.set_xticks(np.arange(start, end, (start+end)/10))\n",
    "\n",
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "years_fmt = mdates.DateFormatter('%Y')\n",
    "\n",
    "# format the ticks\n",
    "ax1.xaxis.set_major_locator(years)\n",
    "ax1.xaxis.set_major_formatter(years_fmt)\n",
    "ax1.xaxis.set_minor_locator(months)\n",
    "\n",
    "# format the coords message box\n",
    "ax1.format_xdata = mdates.DateFormatter('%Y-%m-%d')\n",
    "# ax.grid(True)\n",
    "\n",
    "# round to nearest years.\n",
    "datemin = np.datetime64(str(df_date_sev.index[0]), 'Y')\n",
    "datemax = np.datetime64(str(df_date_sev.index[-1]), 'Y') + np.timedelta64(1, 'Y')\n",
    "ax1.set_xlim(datemin, datemax)\n",
    "plt.setp(ax1.get_xticklabels(), fontsize=14)\n",
    "plt.setp(ax1.get_yticklabels(), fontsize=14)\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_ylabel('Accidents per day\\n',fontsize=14)\n",
    "ax1.set_title('\\nAll Accidents',fontsize=16)\n",
    "ax1.tick_params(which=\"both\", bottom=True, length=6, width=1)\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "ax2 = plt.subplot(212)\n",
    "sns.lineplot(x=df_date_sev2.index, y='Accident_Index',\n",
    "             data=df_date_sev2,ax=ax2,hue='Accident_Severity')\n",
    "# start, end = ax2.set_xlim()\n",
    "# ax2.set_xticks(np.arange(start, end, (start+end)/10))\n",
    "\n",
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "years_fmt = mdates.DateFormatter('%Y')\n",
    "\n",
    "# format the ticks\n",
    "ax2.xaxis.set_major_locator(years)\n",
    "ax2.xaxis.set_major_formatter(years_fmt)\n",
    "ax2.xaxis.set_minor_locator(months)\n",
    "\n",
    "# format the coords message box\n",
    "ax2.format_xdata = mdates.DateFormatter('%Y-%m-%d')\n",
    "ax2.tick_params(which=\"both\", bottom=True, length=6, width=1)\n",
    "\n",
    "# round to nearest years.\n",
    "datemin = np.datetime64(str(df_date_sev2.index[0]), 'Y')\n",
    "datemax = np.datetime64(str(df_date_sev2.index[-1]), 'Y') + np.timedelta64(1, 'Y')\n",
    "ax2.set_xlim(datemin, datemax)\n",
    "plt.setp(ax2.get_xticklabels(), fontsize=14)\n",
    "plt.setp(ax2.get_yticklabels(), fontsize=14)\n",
    "ax2.set_title('\\nHigh Severity Accidents',fontsize=16)\n",
    "ax2.set_xlabel('')\n",
    "ax2.set_ylabel('Accidents per day\\n',fontsize=14)\n",
    "\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('timeline_accidents2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accidents per weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create weekday category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "cat_type = CategoricalDtype(categories=cats, ordered=True)\n",
    "df['Day_of_Week'] = df['Day_of_Week'].astype(cat_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "year_list = [2010,2011,2012,2013,2014,2015,2016,2017]\n",
    "\n",
    "dfdict = {elem : pd.DataFrame() for elem in year_list}\n",
    "\n",
    "for i in year_list:\n",
    "    dfdict[i] = df[df.Year==i].groupby(['Day_of_Week', 'Hour'])['Accident_Index'].count().reset_index().pivot(\n",
    "                            'Day_of_Week', 'Hour', 'Accident_Index')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(4,2,figsize=(16,10),sharey=True)\n",
    "\n",
    "ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8 = axes.ravel()\n",
    "\n",
    "g1 = sns.heatmap(dfdict[2010], cmap='BuPu',cbar=False,ax=ax1)\n",
    "g1.set_title(\"2010 Accidents\")\n",
    "\n",
    "g2 = sns.heatmap(dfdict[2011], cmap='BuPu',cbar=False,ax=ax2)\n",
    "g2.set_title(\"2011 Accidents\")\n",
    "\n",
    "g3 = sns.heatmap(dfdict[2012], cmap='BuPu',cbar=False,ax=ax3)\n",
    "g3.set_title(\"2012 Accidents\")\n",
    "\n",
    "g4 = sns.heatmap(dfdict[2013], cmap='BuPu',cbar=False,ax=ax4)\n",
    "g4.set_title(\"2013 Accidents\")\n",
    "\n",
    "g5 = sns.heatmap(dfdict[2014], cmap='BuPu',cbar=False,ax=ax5)\n",
    "g5.set_title(\"2014 Accidents\")\n",
    "\n",
    "g6 = sns.heatmap(dfdict[2015], cmap='BuPu',cbar=False,ax=ax6)\n",
    "g6.set_title(\"2015 Accidents\")\n",
    "\n",
    "g7 = sns.heatmap(dfdict[2016], cmap='BuPu',cbar=False,ax=ax7)\n",
    "g7.set_title(\"2016 Accidents\")\n",
    "\n",
    "g8 = sns.heatmap(dfdict[2017], cmap='BuPu',cbar=False,ax=ax8)\n",
    "g8.set_title(\"2017 Accidents\")\n",
    "\n",
    "fig.tight_layout()\n",
    "cax,kw = mpl.colorbar.make_axes([ax for ax in axes.flat])\n",
    "plt.colorbar(axes[0][0].get_children()[0], cax=cax, **kw)\n",
    "\n",
    "fig.savefig('accidents_heatmap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "cat_type = CategoricalDtype(categories=cats, ordered=True)\n",
    "df['Day_of_Week'] = df['Day_of_Week'].astype(cat_type)\n",
    "\n",
    "cats = [ 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep','Oct', 'Nov', 'Dec']\n",
    "cat_type = CategoricalDtype(categories=cats, ordered=True)\n",
    "df['Month'] = df['Month'].astype(cat_type)\n",
    "\n",
    "year_list = [2015,2016,2017]\n",
    "\n",
    "dfdictm = {elem : pd.DataFrame() for elem in year_list}\n",
    "\n",
    "for i in year_list:\n",
    "    dfdictm[i] = df[df.Year==i & df.Year].groupby(['Day_of_Week', 'Month'])['Accident_Index'].count().reset_index().pivot(\n",
    "                            'Month', 'Day_of_Week', 'Accident_Index')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_events[acc_events.WeekYear==22].groupby('Year')['NAcc'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_events.Year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_hist = [2015,2016,2017]\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(25,7))\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#acc_events2 = acc_events.copy()\n",
    "#acc_events2['Severity'] = np.where(acc_events2.Accident_Severity.isin(['Serious','Fatal']),'Serious/Fatal','Slight')\n",
    "\n",
    "\n",
    "# r = 1\n",
    "# for i in year_hist:\n",
    "#     plt.subplot(5, 1, r) \n",
    "#     ax = sns.histplot(data=acc_events[acc_events2.Year == i], x='WeekYear', bins=52, kde=False)\n",
    "#                  ,hue='Year', hue_order= ['Serious/Fatal','Slight'], palette={'Slight':'b', 'Serious/Fatal':'r'})\n",
    "#                  #hist_kws=dict(edgecolor=\"k\", linewidth=2), kde=False)   \n",
    "#     ax.set_xlabel('', fontsize=16)\n",
    "#     ax.set_ylabel('',fontsize=16)\n",
    "#     ax.tick_params(labelsize=16)\n",
    "#     plt.title('Weekly Accidents (' + str(i)+')',fontsize=20)\n",
    "#     plt.xlim(1, 53)\n",
    "#     plt.ylim(0, 3800)\n",
    "#     r+=1\n",
    "\n",
    "plt.subplot(1, 1, 1) \n",
    "ax = sns.histplot(data=acc_events, x='WeekYear', bins=52\n",
    "             ,hue='Year', hue_order= [2015,2016,2017], palette={2015:'b', 2016:'r', 2017:'g'}\n",
    "             #,hist_kws=dict(edgecolor=\"k\", linewidth=2, alpha=0.7), \n",
    "             ,kde=False)   \n",
    "ax.set_xlabel('', fontsize=16)\n",
    "ax.set_ylabel('',fontsize=16)\n",
    "ax.tick_params(labelsize=16)\n",
    "plt.title('Weekly Accidents (2015/2016/2017)',fontsize=20)\n",
    "plt.xlim(1, 53)\n",
    "plt.ylim(0, 3800)\n",
    "#r+=1\n",
    "    \n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper left', ncol=1, bbox_to_anchor=(.75, 0.98))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#fig.savefig('weekly_accidents.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1,3,figsize=(16,8),sharey=True)\n",
    "\n",
    "ax1, ax2, ax3 = axes.ravel()\n",
    "\n",
    "g1 = sns.heatmap(dfdictm[2015], cmap='BuPu',cbar=False,ax=ax1)\n",
    "g1.set_title(\"2015 Accidents\")\n",
    "g1.set_xticklabels(g1.get_xticklabels(),fontsize=12)\n",
    "g1.set_yticklabels(g1.get_yticklabels(), rotation=0,fontsize=12)\n",
    "g1.set_xlabel('')\n",
    "g1.set_ylabel('')\n",
    "\n",
    "g2 = sns.heatmap(dfdictm[2016], cmap='BuPu',cbar=False,ax=ax2)\n",
    "g2.set_title(\"2016 Accidents\")\n",
    "g2.set_xticklabels(g2.get_xticklabels(),fontsize=12)\n",
    "g2.set_xlabel('')\n",
    "g2.set_ylabel('')\n",
    "\n",
    "g3 = sns.heatmap(dfdictm[2017], cmap='BuPu',cbar=False,ax=ax3)\n",
    "g3.set_title(\"2017 Accidents\")\n",
    "g3.set_xticklabels(g3.get_xticklabels(),fontsize=12)\n",
    "g3.set_xlabel('')\n",
    "g3.set_ylabel('')\n",
    "\n",
    "fig.tight_layout()\n",
    "cax,kw = mpl.colorbar.make_axes([ax for ax in axes.flat])\n",
    "plt.colorbar(axes[0].get_children()[0], cax=cax, **kw)\n",
    "\n",
    "fig.savefig('accidents_heatmap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_list = [2015,2016,2017]\n",
    "\n",
    "dfdictmt = {elem : pd.DataFrame() for elem in year_list}\n",
    "\n",
    "for i in year_list:\n",
    "    dfdictmt[i] = df[df.Year==i].groupby(['Day_of_Week','Year'])['Accident_Index'].count().reset_index().pivot(\n",
    "                            'Day_of_Week', 'Year', 'Accident_Index')\n",
    "fig, axes = plt.subplots(1,3,figsize=(8,4),sharey=True)\n",
    "\n",
    "ax1, ax2, ax3 = axes.ravel()\n",
    "\n",
    "g1 = sns.heatmap(dfdictmt[2015], cmap='BuPu',cbar=False,ax=ax1)\n",
    "g1.set_title(\"2015 Accidents\")\n",
    "g1.set_xticklabels(g1.get_xticklabels(),fontsize=12)\n",
    "g1.set_yticklabels(g1.get_yticklabels(), rotation=0,fontsize=12)\n",
    "g1.set_xlabel('')\n",
    "g1.set_ylabel('')\n",
    "\n",
    "g2 = sns.heatmap(dfdictmt[2016], cmap='BuPu',cbar=False,ax=ax2)\n",
    "g2.set_title(\"2016 Accidents\")\n",
    "g2.set_xticklabels(g2.get_xticklabels(),fontsize=12)\n",
    "g2.set_xlabel('')\n",
    "g2.set_ylabel('')\n",
    "\n",
    "g3 = sns.heatmap(dfdictmt[2017], cmap='BuPu',cbar=False,ax=ax3)\n",
    "g3.set_title(\"2017 Accidents\")\n",
    "g3.set_xticklabels(g3.get_xticklabels(),fontsize=12)\n",
    "g3.set_xlabel('')\n",
    "g3.set_ylabel('')\n",
    "\n",
    "fig.tight_layout()\n",
    "cax,kw = mpl.colorbar.make_axes([ax for ax in axes.flat])\n",
    "plt.colorbar(axes[0].get_children()[0], cax=cax, **kw)\n",
    "\n",
    "#fig.savefig('accidents_heatmap_overall2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outliers distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlier_y = df.groupby([df.Date,df.Year]).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "sns.boxplot(x='Year', y='Accident_Index', data=df_outlier_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y='Accident_Index', data=df_outlier_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lower end Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_outliers = df_outlier_y[df_outlier_y[col] < df_outlier_y[col].mean() \n",
    "                        - 3 * df_outlier_y[col].std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_outliers.sort_values('Accident_Index', ascending = False)[:5].Date.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Days with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Accident_Index'\n",
    "outliers = df_outlier_y[df_outlier_y[col] > df_outlier_y[col].mean() + 3 * df_outlier_y[col].std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col='Accident_Index'\n",
    "outliers = df_outlier_y[df_outlier_y[col] > df_outlier_y[col].mean() \n",
    "                        + 3 * df_outlier_y[col].std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers['Accident_Index'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.nlargest(30, 'Accident_Index').hist('Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_t = outliers.sort_values('Accident_Index', ascending = False).Date.values\n",
    "df_out = df[df.Date.isin(out_t)] #.groupby(['Local_Authority_(District)']).count().reset_index()\n",
    "df_out = df_out[df_out['Local_Authority_(District)'].isin(df_out['Local_Authority_(District)'].value_counts()[:20].index)]\n",
    "\n",
    "df_outrg = df_out[df_out['Region'].isin(df_out['Region'].value_counts()[:20].index)]\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "chart = sns.countplot(x=df_out['Local_Authority_(District)'],data=df_out,\n",
    "                      order = df_out['Local_Authority_(District)'].value_counts().index)\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=90)\n",
    "plt.title('Car Accidents in '+str(out_t)+' (Top 20 Local Authorities)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1st Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = outliers.sort_values('Accident_Index', ascending = False)[:1].Date.values\n",
    "df_1out = df[df.Date.isin(out1)] #.groupby(['Local_Authority_(District)']).count().reset_index()\n",
    "df_1out = df_1out[df_1out['Local_Authority_(District)'].isin(df_1out['Local_Authority_(District)'].value_counts()[:20].index)]\n",
    "\n",
    "df_1outrg = df_1out[df_1out['Region'].isin(df_1out['Region'].value_counts()[:20].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "chart = sns.countplot(x=df_1out['Local_Authority_(District)'],data=df_1out,order = df_1out['Local_Authority_(District)'].value_counts().index)\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=90)\n",
    "plt.title('Car Accidents in '+str(out1)+' (Top 20 Local Authorities)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "chart = sns.countplot(x=df_1outrg['Region'],data=df_1outrg,order = df_1outrg['Region'].value_counts().index)\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=90)\n",
    "plt.title('Car Accidents in '+str(out1)+' (Top 20 Regions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2nd Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = outliers.sort_values('Accident_Index', ascending = False)[1:2].Date.values\n",
    "df_2out = df[df.Date.isin(out2)] #.groupby(['Local_Authority_(District)']).count().reset_index()\n",
    "df_2out = df_2out[df_2out['Local_Authority_(District)'].isin(df_2out['Local_Authority_(District)'].value_counts()[:20].index)]\n",
    "\n",
    "df_2outrg = df_2out[df_2out['Region'].isin(df_2out['Region'].value_counts()[:20].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "chart = sns.countplot(x=df_2out['Local_Authority_(District)'],data=df_2out,order = df_2out\n",
    "                      ['Local_Authority_(District)'].value_counts().index)\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=90)\n",
    "plt.title('Car Accidents in '+str(out2)+' (Top 20 Local Authorities)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "chart = sns.countplot(x=df_2outrg['Region'],data=df_2outrg,order = df_2outrg['Region'].value_counts().index)\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=90)\n",
    "plt.title('Car Accidents in '+str(out2)+' (Top 20 Regions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3rd Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out3 = outliers.sort_values('Accident_Index', ascending = False)[2:3].Date.values\n",
    "df_3out = df[df.Date.isin(out3)] #.groupby(['Local_Authority_(District)']).count().reset_index()\n",
    "df_3out = df_3out[df_3out['Local_Authority_(District)'].isin(df_3out['Local_Authority_(District)'].value_counts()[:20].index)]\n",
    "\n",
    "df_3outrg = df_3out[df_3out['Region'].isin(df_3out['Region'].value_counts()[:20].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "chart = sns.countplot(x=df_3out['Local_Authority_(District)'],data=df_3out,order = df_3out['Local_Authority_(District)'].value_counts().index)\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=90)\n",
    "plt.title('Car Accidents in '+str(out3)+' (Top 20 Local Authorities)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "chart = sns.countplot(x=df_3outrg['Region'],data=df_3outrg,order = df_3outrg['Region'].value_counts().index)\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=90)\n",
    "plt.title('Car Accidents in '+str(out3)+' (Top 20 Regions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers in Birmingham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birg_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birg_out = outliers.sort_values('Accident_Index', ascending = False)[:3].Date.values\n",
    "df[(df.Date.isin(birg_out)) & (df['Local_Authority_(District)']=='Birmingham')].groupby(\n",
    "    ['Accident_Severity','Day_of_Week'])['Accident_Index'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of Birmingham accidents (Serious/Fatal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out10s = outliers.sort_values('Accident_Index', ascending = False)[:10].Date.values\n",
    "df_10out = df[df.Date.isin(out10s) & (df['Local_Authority_(District)']=='Birmingham') &\n",
    "              (df.Accident_Severity!='Slight')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10out['Accident_Index'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "ax = sns.barplot(x=\"Date\", y=\"Number_of_Casualties\", hue=\"Accident_Severity\", data=df_10out)\n",
    "#plt.plot(x='Date',y='Number_of_Casualties',data=df_10out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "ax = sns.barplot(x=\"Date\", y=\"Number_of_Casualties\", hue=\"Hour\", data=df_10out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad-hoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = df[((df.Date>='2015-01-01') & (df.Date<='2017-12-31'))\n",
    "                  & (df.Region=='London')\n",
    "                  & (df.Number_of_Casualties>1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "chart = sns.countplot(x=b['Local_Authority_(District)'],\n",
    "                      data=b,order = b['Local_Authority_(District)'].value_counts().index)\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=90)\n",
    "#plt.title('Car Accidents in '+str(out3)+' (Top 20 Local Authorities)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of dates in each year (2013-2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_events['NAcc'] = 1\n",
    "acc_events[(acc_events.Month=='Jan') & (acc_events.Year==2015)].groupby('Date')['NAcc'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_analysis = df[df.Year >= 2013]\n",
    "acc_events = df[df.Year >= 2013]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#year_hist = [2013,2014,2015,2016,2017]\n",
    "\n",
    "year_hist = [2015,2016,2017]\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "acc_events2 = acc_events.copy()\n",
    "acc_events2['Severity'] = np.where(acc_events2.Accident_Severity.isin(['Serious','Fatal']),'Serious/Fatal','Slight')\n",
    "\n",
    "\n",
    "r = 1\n",
    "for i in year_hist:\n",
    "    plt.subplot(5, 1, r) \n",
    "    ax = sns.histplot(data=acc_events2[acc_events2.Year == i], x='WeekYear', bins=52, kde=False)\n",
    "                 #,hue='Severity', hue_order= ['Serious/Fatal','Slight'], palette={'Slight':'b', 'Serious/Fatal':'r'})\n",
    "                 #hist_kws=dict(edgecolor=\"k\", linewidth=2), kde=False)   \n",
    "    plt.title('Weekly Accidents in ' + str(i))\n",
    "    plt.xlim(1, 53)\n",
    "    plt.ylim(0, 3800)\n",
    "    r+=1\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper left', ncol=1, bbox_to_anchor=(.75, 0.98))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Times of the day x Number of casualties (year by year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_events.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#year_hist = [2013,2014,2015,2016,2017]\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# ax = sns.scatterplot(data=acc_events,\n",
    "#                 x='Hour',y='Number_of_Casualties',hue='Year',alpha=0.7,\n",
    "#                 palette=\"colorblind\")\n",
    "\n",
    "ax = sns.relplot(data=acc_events_2,\n",
    "                x='Hour',y='Number_of_Casualties',hue='Year',alpha=0.7,\n",
    "                palette=\"colorblind\",kind='scatter',col='Severity'\n",
    ")\n",
    "\n",
    "FacetGrid.set(xticks=np.arange(0,24,1))\n",
    "\n",
    "#ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.set_xlim(-0.5,24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp = df[(df.Region=='London') & (df.Year>=2006)]\n",
    "#df_temp = df[(df.Accident_Severity!='Slight') & (df.Year>=2005)]\n",
    "df_temp = df[(df.Accident_Severity!='Slight') & (df.Year>=2005)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lightness'] = np.where(df.Light_Conditions=='Daylight','Daylight',\n",
    "                           np.where(df.Light_Conditions.str.contains('Darkness'),'Darkness',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Month.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_h = df_temp.groupby(['Year','Month','Day_of_Week','Hour']).count().reset_index()\n",
    "df_temp_h = df_temp_h.groupby('Hour').mean().reset_index()\n",
    "df_temp_h.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_y = df_temp.groupby(['Year']).count().reset_index()\n",
    "\n",
    "# df_temp_m = df_temp.groupby(['Year','Month','Lightness']).count().reset_index()\n",
    "# df_temp_m = df_temp_m.groupby(['Month','Lightness']).mean().reset_index()\n",
    "\n",
    "df_temp_m = df_temp.groupby(['Year','Month']).count().reset_index()\n",
    "df_temp_m = df_temp_m.groupby(['Month']).mean().reset_index()\n",
    "\n",
    "df_temp_w = df_temp.groupby(['Year','Month','Day_of_Week']).count().reset_index()\n",
    "df_temp_w = df_temp_w.groupby('Day_of_Week').mean().reset_index()\n",
    "\n",
    "df_temp_h = df_temp.groupby(['Year','Month','Day_of_Week','Hour']).count().reset_index()\n",
    "df_temp_h = df_temp_h.groupby('Hour').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "f, axes = plt.subplots(2, 2, figsize=(18,9))\n",
    "\n",
    "Months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "             'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "Weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', \n",
    "           'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "sns.barplot(x='Year',y='Accident_Index',data=df_temp_y,ax=axes[0][0])\n",
    "sns.barplot(x='Month',y='Accident_Index',data=df_temp_m,order=Months,ax=axes[0][1])\n",
    "#sns.barplot(x='Month',y='Accident_Index',data=df_temp_m,order=Months,ax=axes[0][1],hue='Lightness')\n",
    "sns.barplot(x='Day_of_Week',y='Accident_Index',data=df_temp_w,order=Weekdays,ax=axes[1][0])\n",
    "sns.barplot(x='Hour',y='Accident_Index',data=df_temp_h,ax=axes[1][1])\n",
    "\n",
    "axes[0][0].set_title('Number of Accidents per Year')\n",
    "axes[0][1].set_title('Avg of Accidents by Month (2005-2017)')\n",
    "axes[1][0].set_title('Avg of Accidents by Day of Week (2005-2017)')\n",
    "axes[1][1].set_title('Avg of Accidents by Hour (2005-2017)')\n",
    "\n",
    "axes[0][0].set_ylabel('')\n",
    "axes[0][1].set_ylabel('')\n",
    "axes[1][0].set_ylabel('')\n",
    "axes[1][1].set_ylabel('')\n",
    "\n",
    "axes[1][0].set_xlabel('Year')\n",
    "axes[1][0].set_xlabel('Day of Week')\n",
    "axes[1][0].set_xlabel('Day of Week')\n",
    "axes[1][0].set_xlabel('Day of Week')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#f.savefig('fig12c.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_events_201301['Latitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr_pol_en = acc_events_201301[acc_events_201301.Country=='England']['Latitude']\n",
    "ctr_pol_wa = acc_events_201301[acc_events_201301.Country=='Wales']['Latitude']\n",
    "ctr_pol_sc = acc_events_201301[acc_events_201301.Country=='Scotland']['Latitude']\n",
    "\n",
    "\n",
    "hr_pol_en = acc_events_201301[acc_events_201301.Country=='England']['Hour']\n",
    "hr_pol_wa = acc_events_201301[acc_events_201301.Country=='Wales']['Hour']\n",
    "hr_pol_sc = acc_events_201301[acc_events_201301.Country=='Scotland']['Hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set up random data between 0 and 90\n",
    "#r = [np.random.random() * 90.0 for i in range(0,10)]\n",
    "#r = .tolist()\n",
    "\n",
    "# set up 24 hours matching the random data above\n",
    "#hours = np.linspace(0.0,24.0,len(r))\n",
    "hours_en = np.array(hr_pol_en)\n",
    "hours_wa = np.array(hr_pol_wa)\n",
    "hours_sc = np.array(hr_pol_sc)\n",
    "\n",
    "# scaling the 24 hours to the full circle, 2pi\n",
    "theta_en = hours_en / 24.0 * (2.0 * np.pi)\n",
    "theta_wa = hours_wa / 24.0 * (2.0 * np.pi)\n",
    "theta_sc = hours_sc / 24.0 * (2.0 * np.pi)\n",
    "\n",
    "\n",
    "# reverse your data, so that 90 becomes 0:\n",
    "#r_rev = [(ri - 90.0) * -1.0 for ri in r]\n",
    "r_rev_en = ctr_pol_en.tolist()\n",
    "r_rev_wa = ctr_pol_wa.tolist()\n",
    "r_rev_sc = ctr_pol_sc.tolist()\n",
    "\n",
    "plt.figure(figsize=(22, 12))\n",
    "# set up your polar plot\n",
    "ax = plt.subplot(111, projection='polar')\n",
    "ax.plot(theta_en, r_rev_en, color='r', linewidth=0.40, alpha=0.6)\n",
    "ax.plot(theta_wa, r_rev_wa, color='k', linewidth=0.50, alpha=0.8)\n",
    "ax.plot(theta_sc, r_rev_sc, color='b', linewidth=0.40, alpha=0.7)\n",
    "\n",
    "# define your axis limits\n",
    "ax.set_ylim([40.0, 61.0])\n",
    "\n",
    "# statically reverse your y-tick-labels\n",
    "# caution: this turns your labels into strings\n",
    "#          and decouples them from the data\n",
    "# \n",
    "# the np.linspace gives you a distribution between 90 and 0 -\n",
    "# the number of increments are related to the number of ticks\n",
    "# however, you require one more label, because the center is \n",
    "#     omitted.  \n",
    "ax.set_yticklabels(['{:.0f}'.format(ylabel) \\\n",
    "                for ylabel in np.linspace(40.0,61.0,len(ax.get_yticklabels())+1)[1:]])\n",
    "\n",
    "\n",
    "# statically turn your x-tick-labels into fractions of 24\n",
    "# caution: this turns your labels into strings\n",
    "#          and decouples them from the data\n",
    "#\n",
    "# the number of ticks around the polar plot is used to derive\n",
    "#    the appropriate increment for the 24 hours\n",
    "ax.set_xticklabels(['{:.1f}'.format(xlabel) \\\n",
    "                    for xlabel in np.arange(0.0,24.0,(24.0 / len(ax.get_xticklabels())))])\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df_dt, #the dataframe to pull from\n",
    "                  row=\"Year\", #define the column for each subplot row to be differentiated by\n",
    "                  hue=\"Year\", #define the column for each subplot color to be differentiated by\n",
    "                  aspect=5, #aspect * height = width\n",
    "                  height=1.5, #height of each subplot\n",
    "                  palette=['#4285F4','#EA4335','#FBBC05','#34A853'] #google colors\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.map(sns.kdeplot, \"Accident_Index\", shade=True, alpha=1, lw=1.5, bw_method=0.2)\n",
    "g.map(sns.kdeplot, \"Accident_Index\", lw=4, bw=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spatial distribution of May/2020 accidents considering daily events with maximum duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_jun20 = df.groupby('Date')[['State','Start_Lat','Start_Lng','Duration_Minutes']].agg(\n",
    "    {'Duration_Minutes':['max'], 'State':['first'],\n",
    "    'Start_Lat':['first'],'Start_Lng':['first']}).reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_jun20 = df_events_jun20.loc['2020-01-01':'2020-02-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_jun20.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[45.5236, -122.6750])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add markers to map\n",
    "for lat, lng, state in zip(df_events_jun20['Start_Lat'], df_events_jun20['Start_Lng'], df_events_jun20['State']):\n",
    "    label = 'Name: {0}'.format(state)\n",
    "    iframe = folium.IFrame(html=label, width=300, height=100)\n",
    "    popup = folium.Popup(iframe, parse_html=True)\n",
    "    folium.CircleMarker([lat, lng],\n",
    "                        radius=5,\n",
    "                        popup=popup,\n",
    "                        color='blue',\n",
    "                        fill=True,\n",
    "                        fill_color='blue',\n",
    "                        fill_opacity=0.7,\n",
    "                       ).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step to linear using .to_linear()\n",
    "colorscale = branca.colormap.step.RdYlBu_11.to_linear().scale(0, 30)\n",
    "colorscale.caption = 'Color Scale'\n",
    "def style_function(feature):\n",
    "    employed = employed_series.get(int(feature['id'][-5:]), None)\n",
    "    return {\n",
    "        'fillOpacity': 0.5,\n",
    "        'weight': 0,\n",
    "        'fillColor': '#black' if employed is None else colorscale(employed)\n",
    "    }\n",
    "us_county_lin = folium.Map(\n",
    "    location=[42, -100],\n",
    "    tiles='cartodbpositron',\n",
    "    zoom_start=4\n",
    ")\n",
    "folium.TopoJson(\n",
    "    json.loads(requests.get(counties).text),\n",
    "    'objects.us_counties_20m',\n",
    "    style_function=style_function\n",
    ").add_to(us_county_lin)\n",
    "colorscale.add_to(us_county_lin)\n",
    "us_county_lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial distribution of accidents events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basedate = pd.to_datetime(acc_events.Date.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc_events['days_since'] = (acc_events['Datetime'] - pd.to_datetime(acc_events.Date.min())).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[((df.Date>='2015-01-01') & (df.Date<='2017-12-31'))\n",
    "#                   & (df.Region=='London')\n",
    "#                   & (df.Number_of_Casualties>1)].groupby(['Local_Authority_(District)']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = folium.Map(location=[54.584797 , -3.438721], zoom_start=6,\n",
    "               tiles='cartodbpositron', width='100%', height='100%') \n",
    "# If you adjusted the notebook display width to be as wide as your screen, the map might get very big. \n",
    "# Adjust size as desired.\n",
    "\n",
    "\n",
    "\n",
    "for id, row in df[((df.Date>='2015-10-01') & (df.Date<='2017-12-31'))\n",
    "                  & (df.Number_of_Casualties>1)].iterrows():\n",
    "    #if row['Accident_Severity']!='Slight':\n",
    "#         label = 'Name: {0}'.format(row['Datetime'])\n",
    "#         iframe = folium.IFrame(html=label, width=300, height=100)\n",
    "#         popup = folium.Popup(iframe, parse_html=True)\n",
    "        folium.CircleMarker((row.Latitude, row.Longitude),\n",
    "                            radius=row.Number_of_Casualties*0.2, color='b', \n",
    "                            fill=True, fill_opacity=0.6, #opacity=0,\n",
    "                            fill_color='red').add_to(m)\n",
    "    #else:\n",
    "    #    folium.CircleMarker((row.Latitude, row.Longitude),\n",
    "    #                    radius=row.Number_of_Casualties*0.2, color='b', \n",
    "    #                        fill=True, fill_opacity=0.6, #opacity=0,  \n",
    "    #                   fill_color='blue').add_to(m)\n",
    "    \n",
    "    \n",
    "    \n",
    "# for id, row in df[((df.Date>='2015-01-01') & (df.Date<='2015-03-31'))\n",
    "#               & (df.Accident_Severity=='Slight')\n",
    "#               & (df.Number_of_Casualties>1)].iterrows():\n",
    "#     folium.CircleMarker((row.Latitude, row.Longitude),\n",
    "#                     radius=2, color='b', fill=True, fill_opacity=0.5, #opacity=0,  \n",
    "#                     fill_color='blue').add_to(m)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "m\n",
    "\n",
    "#m.save(os.path.join('', 'f_3yn_accidents.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "delay=5\n",
    " \n",
    "#Save the map as an HTML file\n",
    "fn='testmap.html'\n",
    "tmpurl='file://{path}/{mapfile}'.format(path=os.getcwd(),mapfile=fn)\n",
    "m.save(fn)\n",
    "\n",
    "#Open a browser window...\n",
    "browser = webdriver.Chrome()\n",
    "#..that displays the map...\n",
    "browser.get(tmpurl)\n",
    "#Give the map tiles some time to load\n",
    "time.sleep(delay)\n",
    "#Grab the screenshot\n",
    "browser.save_screenshot('map2.png')\n",
    "#Close the browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large clusters can be seen around Greater London, Birmingham, Manchester, Leeds, Newcastle upton Tyne and Edinburgh and Glasgow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_lon = dflon.sort_values(by=['Accident_Index'],ascending=False).nlargest(10,'Accident_Index')['Date'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflon = df[((df.Date>='2010-01-01')) #& (df.Date<='2017-12-31'))\n",
    "                  & (df.Region=='London')].groupby('Date')['Accident_Index'].count().reset_index()\n",
    "\n",
    "\n",
    "dflon.plot('Date','Accident_Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[51.484797 , -0.138721],\n",
    "               zoom_start=10,\n",
    "               tiles='cartodbpositron', width='100%', height='100%') \n",
    "# If you adjusted the notebook display width to be as wide as your screen, the map might get very big. \n",
    "# Adjust size as desired.\n",
    "\n",
    "for id, row in df[((df.Date=='2016-11-25')) #& (df.Date<='2017-12-31'))\n",
    "                  & (df.Region=='London')].iterrows():\n",
    "    #if row['Accident_Severity']!='Slight':\n",
    "        label = 'Date: {0}'.format(row['Datetime'])\n",
    "        iframe = folium.IFrame(html=label, width=300, height=100)\n",
    "        popup = folium.Popup(iframe, parse_html=True)\n",
    "        folium.CircleMarker((row.Latitude, row.Longitude),\n",
    "                            radius=3, color='b', fill=True, fill_opacity=1, popup=popup, #opacity=0,\n",
    "                            fill_color='red').add_to(m)\n",
    "    #else:\n",
    "    #    folium.CircleMarker((row.Latitude, row.Longitude),\n",
    "    #                    radius=3, color='b', fill=True, fill_opacity=1, #opacity=0,  \n",
    "    #                   fill_color='blue').add_to(m)\n",
    "\n",
    "    \n",
    "# sw = df[['Latitude', 'Longitude']].min().values.tolist()\n",
    "# ne = df[['Latitude', 'Longitude']].max().values.tolist()\n",
    "\n",
    "# m.fit_bounds([sw, ne]) \n",
    "    \n",
    "m\n",
    "\n",
    "#m.save(os.path.join('', 'f_trim_2013_lon_accidents.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the map as an HTML file\n",
    "fn='testmap.html'\n",
    "tmpurl='file://{path}/{mapfile}'.format(path=os.getcwd(),mapfile=fn)\n",
    "m.save(fn)\n",
    "\n",
    "#Open a browser window...\n",
    "browser = webdriver.Chrome()\n",
    "#..that displays the map...\n",
    "browser.get(tmpurl)\n",
    "#Give the map tiles some time to load\n",
    "time.sleep(delay)\n",
    "#Grab the screenshot\n",
    "browser.save_screenshot('mapldn.png')\n",
    "#Close the browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[((df.Date>='2005-01-01')) & (df.Date<='2017-12-31')\n",
    "                  & (df.Region=='London')\n",
    "                  & (df.Accident_Severity=='Fatal')\n",
    "                  & (df.Number_of_Casualties>1)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from folium.features import DivIcon\n",
    "\n",
    "\n",
    "m = folium.Map(location=[51.504797 , -0.068721],\n",
    "               zoom_start=12,\n",
    "               tiles='cartodbpositron', width='100%', height='100%') \n",
    "# If you adjusted the notebook display width to be as wide as your screen, the map might get very big. \n",
    "# Adjust size as desired.\n",
    "\n",
    "\n",
    "df_lon_mul = df[((df.Date>='2015-01-01') & (df.Date<='2017-12-31'))\n",
    "                  & (df.Region=='London')\n",
    "                  & (df.Number_of_Casualties>1)]\n",
    "df_lon_mul=df_lon_mul.groupby(['Hour'])['Accident_Index'].agg(\n",
    "    # make the numbers numeric otherwise it just concatenates strings\n",
    "    lambda x: pd.to_numeric(x, errors='coerce').count()\n",
    ")\n",
    "\n",
    "hour_dict = df_lon_mul.to_dict()\n",
    "\n",
    "colormap = branca.colormap.linear.YlOrRd_09.scale(0, 24)\n",
    "colormap = colormap.to_step(index=[0, 8, 14, 20, 24])\n",
    "colormap.caption = 'Hours of Accidents in London (2015-2017)'\n",
    "#'2015-10-01'\n",
    "#text = 'Test'\n",
    "for id, row in df[((df.Date>='2005-01-01')) & (df.Date<='2017-12-31')\n",
    "                  & (df.Region=='London')\n",
    "                  & (df.Accident_Severity=='Fatal')\n",
    "                  & (df.Number_of_Casualties>1)\n",
    "                 ].iterrows():\n",
    "        folium.CircleMarker((row.Latitude, row.Longitude),\n",
    "                            radius=row.Number_of_Casualties, \n",
    "                            popup='Date of the accident:'+str(row.Datetime),\n",
    "                            fill=True,\n",
    "                            color='b',\n",
    "                            fill_color = colormap(colormap.index[1] if ((row.Hour>=0) & (row.Hour<8)) \n",
    "                                                  else colormap.index[2]\n",
    "                                                  if (row.Hour>=8) & (row.Hour<14) \n",
    "                                                  else colormap.index[3]\n",
    "                                                  if (row.Hour>=14) & (row.Hour<20) \n",
    "                                                  else colormap.index[4])\n",
    "                            ,fill_opacity=1).add_to(m)\n",
    "        \n",
    "#         folium.map.Marker(\n",
    "#                         (row.Latitude, row.Longitude),\n",
    "#                         icon=DivIcon(\n",
    "#                             icon_size=(150,36),\n",
    "#                             icon_anchor=(0,0),\n",
    "#                             html='<div style=\"font-size: 12pt\">%s</div>' % str('Date of the accident:'+str(row.Datetime)),\n",
    "#                             )\n",
    "#                         ).add_to(m)\n",
    "        \n",
    "m.add_child(colormap)\n",
    "        \n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.features import DivIcon\n",
    "\n",
    "\n",
    "m = folium.Map(location=[51.504797 , -0.068721],\n",
    "               zoom_start=12,\n",
    "               tiles='cartodbpositron', width='100%', height='100%') \n",
    "# If you adjusted the notebook display width to be as wide as your screen, the map might get very big. \n",
    "# Adjust size as desired.\n",
    "\n",
    "\n",
    "df_lon_mul = df[((df.Date>='2015-01-01') & (df.Date<='2017-12-31'))\n",
    "                  & (df.Region=='London')\n",
    "                  & (df.Number_of_Casualties>1)]\n",
    "df_lon_mul=df_lon_mul.groupby(['Hour'])['Accident_Index'].agg(\n",
    "    # make the numbers numeric otherwise it just concatenates strings\n",
    "    lambda x: pd.to_numeric(x, errors='coerce').count()\n",
    ")\n",
    "\n",
    "hour_dict = df_lon_mul.to_dict()\n",
    "\n",
    "colormap = branca.colormap.linear.YlOrRd_09.scale(0, 24)\n",
    "colormap = colormap.to_step(index=[0, 8, 14, 20, 24])\n",
    "colormap.caption = 'Hours of Accidents in London (2015-2017)'\n",
    "#'2015-10-01'\n",
    "#text = 'Test'\n",
    "for id, row in df[((df.Date>='2017-01-01')) & (df.Date<='2017-12-31')\n",
    "                  & (df.Region=='London')\n",
    "                  #& (df.Accident_Severity=='Serious')\n",
    "                 ].iterrows():\n",
    "        folium.CircleMarker((row.Latitude, row.Longitude),\n",
    "                            radius=row.Number_of_Casualties, \n",
    "                            popup='Date of the accident:'+str(row.Datetime),\n",
    "                            fill=True,\n",
    "                            color='b',\n",
    "                            fill_color='b'\n",
    "#                             fill_color = colormap(colormap.index[1] if ((row.Hour>=0) & (row.Hour<8)) \n",
    "#                                                   else colormap.index[2]\n",
    "#                                                   if (row.Hour>=8) & (row.Hour<14) \n",
    "#                                                   else colormap.index[3]\n",
    "#                                                   if (row.Hour>=14) & (row.Hour<20) \n",
    "#                                                   else colormap.index[4])\n",
    "                            ,fill_opacity=1).add_to(m)\n",
    "        \n",
    "#         folium.map.Marker(\n",
    "#                         (row.Latitude, row.Longitude),\n",
    "#                         icon=DivIcon(\n",
    "#                             icon_size=(150,36),\n",
    "#                             icon_anchor=(0,0),\n",
    "#                             html='<div style=\"font-size: 12pt\">%s</div>' % str('Date of the accident:'+str(row.Datetime)),\n",
    "#                             )\n",
    "#                         ).add_to(m)\n",
    "        \n",
    "m.add_child(colormap)\n",
    "        \n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the map as an HTML file\n",
    "fn='testmap.html'\n",
    "tmpurl='file://{path}/{mapfile}'.format(path=os.getcwd(),mapfile=fn)\n",
    "m.save(fn)\n",
    "\n",
    "#Open a browser window...\n",
    "browser = webdriver.Chrome()\n",
    "#..that displays the map...\n",
    "browser.get(tmpurl)\n",
    "#Give the map tiles some time to load\n",
    "time.sleep(delay)\n",
    "#Grab the screenshot\n",
    "browser.save_screenshot('mapldn_hr2.png')\n",
    "#Close the browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lon_mul = df[((df.Date>='2015-01-01') & (df.Date<='2017-12-31'))\n",
    "                  & (df.Region=='London')\n",
    "                  & (df.Number_of_Casualties>1)]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "n, bins, patches = plt.hist(df_lon_mul['Hour'], 23)\n",
    "\n",
    "for c, p in zip(bins, patches):\n",
    "    if round(c,0) >= 0 and round(c,0) < 8:\n",
    "        plt.setp(p, 'facecolor', '#ffffccff')\n",
    "    elif round(c,0) >= 8 and round(c,0) < 14:\n",
    "        plt.setp(p, 'facecolor', '#fea646ff')\n",
    "    elif round(c,0) >= 14 and round(c,0) < 20:\n",
    "        plt.setp(p, 'facecolor', '#e31a1cff')\n",
    "    else:\n",
    "        plt.setp(p, 'facecolor', '#800026ff')\n",
    "\n",
    "plt.xticks(np.arange(0, 24, step=1))\n",
    "plt.xlim(0,23)\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "fig.savefig('histldn_hr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed limit map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "m = folium.Map(location=[51.504797 , -0.068721],\n",
    "               zoom_start=11,\n",
    "               tiles='cartodbpositron', width='100%', height='100%') \n",
    "# If you adjusted the notebook display width to be as wide as your screen, the map might get very big. \n",
    "# Adjust size as desired.\n",
    "\n",
    "colormap = cm.LinearColormap(colors=['#FFD0C2','#FF8A83','#D65F59','#C23210','#991101','#680101'], \n",
    "                             index=[20, 30, 40, 50, 60, 70],vmin=20,vmax=70)\n",
    "colormap = colormap.to_step(index=[20, 30, 40, 50, 60, 70])\n",
    "colormap.caption = 'Road Speed limit of Accidents in London (2015-2017)'\n",
    "\n",
    "for id, row in df[((df.Date>='2015-01-01') & (df.Date<='2017-12-31'))\n",
    "                  & (df.Region=='London')\n",
    "                  & (df.Number_of_Casualties>1)].iterrows():\n",
    "        folium.CircleMarker((row.Latitude, row.Longitude),\n",
    "                            radius=row.Number_of_Casualties, color='b',\n",
    "                            popup='Speed limit of the road:'+str(row.Speed_limit),\n",
    "                            fill_color = '#FFD0C2' if row.Speed_limit==20 else '#FF8A83' if row.Speed_limit==30 else '#D65F59' \n",
    "                            if row.Speed_limit==40 else '#C23210' if row.Speed_limit==50 else\n",
    "                            '#991101' if row.Speed_limit==60 else '#680101'\n",
    "                            ,fill_opacity=0.4).add_to(m)\n",
    "\n",
    "m.add_child(colormap)\n",
    "        \n",
    "#m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the map as an HTML file\n",
    "fn='testmap.html'\n",
    "tmpurl='file://{path}/{mapfile}'.format(path=os.getcwd(),mapfile=fn)\n",
    "m.save(fn)\n",
    "\n",
    "#Open a browser window...\n",
    "browser = webdriver.Chrome()\n",
    "#..that displays the map...\n",
    "browser.get(tmpurl)\n",
    "#Give the map tiles some time to load\n",
    "time.sleep(delay)\n",
    "#Grab the screenshot\n",
    "browser.save_screenshot('mapldn_spd.png')\n",
    "#Close the browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lon_mul = df[((df.Date>='2015-01-01') & (df.Date<='2017-12-31'))\n",
    "                  & (df.Region=='London')\n",
    "                  & (df.Number_of_Casualties>1)]\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(16, 6))\n",
    "\n",
    "ax = sns.countplot(x='Speed_limit', data=df_lon_mul,\n",
    "                   palette=['#FFD0C2','#FF8A83','#D65F59',\n",
    "                            '#C23210','#991101','#680101'])\n",
    "\n",
    "# for c, p in zip(bins, patches):\n",
    "#     print(c,p)\n",
    "#     if c==20:\n",
    "#         plt.setp(p, 'facecolor', '#FFD0C2')\n",
    "#     elif c==30:\n",
    "#         plt.setp(p, 'facecolor', '#FF8A83')\n",
    "#     elif c==40:\n",
    "#         plt.setp(p, 'facecolor', '#D65F59')\n",
    "#     if c==50:\n",
    "#         plt.setp(p, 'facecolor', '#C23210')\n",
    "#     elif c==60:\n",
    "#         plt.setp(p, 'facecolor', '#991101')\n",
    "#     else:\n",
    "#         plt.setp(p, 'facecolor', '#680101')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('histldn_spd.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the data on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_events_201301 = acc_events[((acc_events.Date>='2013-01-01') &\n",
    "                               (acc_events.Date<'2013-02-01')) & (acc_events.Number_of_Casualties > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_events_201301.Number_of_Casualties.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_events_2014.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c is the attribute we'll map onto colors, s is the attribute we'll represent with circle size.\n",
    "acc_events_201301.plot(kind='scatter', x='Longitude', y='Latitude',\n",
    "    s=acc_events_201301['Number_of_Casualties']*20, label='Number_of_Casualties',\n",
    "    c='Hour', cmap=plt.get_cmap(\"jet\"),\n",
    "    colorbar=True, alpha=0.5, figsize=(10,10),\n",
    ")\n",
    "plt.legend()\n",
    "#save_fig(\"housing_prices_scatterplot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the spatial distribution in 3D (2D space and 1D Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(1, figsize=(9, 6))\n",
    "\n",
    "acc_events_2014 = acc_events[(acc_events.Year==2014)]\n",
    "\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 5], elev=25, azim=115) # change parameters here for looking from a different perspective\n",
    "ax.scatter(acc_events_2014['Longitude'], acc_events_2014['Latitude'], \n",
    "           acc_events_2014['days_since'] #,c=acc_events_2014['days_since'], cmap='Greens'\n",
    "           , alpha=0.1, c=acc_events_2014['days_since'], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default configuration for DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn DBSCAN haversine distance metric requires data in the form of [latitude, longitude] and both inputs and outputs are in units of radians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# define the number of kilometers in one radian\n",
    "kms_per_radian = 6371.0088\n",
    "#spatial_dist_max = 5 / kms_per_radian\n",
    "#temporal_dist_max = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* eps is the physical distance from each point that forms its neighborhood\n",
    "* min_samples is the min cluster size, otherwise it's noise - set to 1 so we get no noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# represent points consistently as (lat, lon)\n",
    "coords = acc_events_201301[['Longitude','Latitude']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### spatial_dist_max = 25 / kms_per_radian\n",
    "temporal_dist_max = 24\n",
    "n_neighbours = 3\n",
    "\n",
    "# define epsilon as 1.5 kilometers, converted to radians for use by haversine\n",
    "eps = 1.5 / kms_per_radian\n",
    "\n",
    "clustered_ST = DBSCAN(eps=eps,metric='haversine', \n",
    "                      min_samples=n_neighbours).fit(np.radians(coords))\n",
    "\n",
    "print(\"Clustering finished!\")\n",
    "\n",
    "labels=clustered_ST.labels_\n",
    "unique_labels=np.unique(clustered_ST.labels_)\n",
    "print('Result: {} records in the noise, labelled as -1, and {} clusters labelled as 0..{}'.\n",
    "      format(acc_events_201301[labels==-1].shape[0], len(unique_labels)-1, len(unique_labels)-2))\n",
    "#clustered\n",
    "clust_id_col_name='ClusterN'\n",
    "acc_events_201301[clust_id_col_name]=labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting cluster sizes\n",
    "cluster_sizes = acc_events_201301[clust_id_col_name].value_counts().rename_axis('Cluster id').to_frame('count')\n",
    "print(\"Cluster sizes:\")\n",
    "print(cluster_sizes.head(15))\n",
    "print(\"...\")\n",
    "print(cluster_sizes.tail(15))\n",
    "\n",
    "cluster_sizes = cluster_sizes[cluster_sizes.index != -1] # no noise\n",
    "\n",
    "max_cluster_size=cluster_sizes['count'].max()\n",
    "print(\"max = \",max_cluster_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_func = {\n",
    "    'days_since':['max','min'],\n",
    "    'Longitude':['mean','max','min'],\n",
    "    'Latitude':['mean','max','min']\n",
    "}\n",
    "st_aggregates = acc_events_201301.reset_index(drop=False)[['ClusterN','days_since',\n",
    "                                                'Longitude','Latitude']].groupby(['ClusterN']).agg(agg_func)\n",
    "# Flatten hierarchical column names\n",
    "st_aggregates.columns = [\"_\".join(x) for x in st_aggregates.columns.ravel()]\n",
    "# compute derived attributes: duration and bounding rectangle diagonal\n",
    "st_aggregates['duration (days)']=st_aggregates['days_since_max']-st_aggregates['days_since_min']\n",
    "for id,row in st_aggregates.iterrows():\n",
    "    brd=kms_per_radian*great_circle2(row['Latitude_max'],row['Longitude_max'],row['Latitude_min'],row['Longitude_min'])\n",
    "    #print('{}'.format(brd))\n",
    "    #print(row['Latitude_max'],row['Longitude_max'],row['Latitude_min'],row['Longitude_min'])\n",
    "    st_aggregates.at[id,'Bound_rect_diag(km)']=brd\n",
    "st_aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clusters_data = st_aggregates.loc[st_aggregates.index!=-1,\n",
    "                                  ['Latitude_mean','Longitude_mean',\n",
    "                                   'days_since_min','days_since_max']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "clusters_data_scaled = scaler.fit_transform(clusters_data)\n",
    "\n",
    "mds_ST = MDS(n_components = 2, random_state=110)\n",
    "mds_ST.fit(clusters_data_scaled)\n",
    "xy_mds_ST = mds_ST.fit_transform(clusters_data_scaled)\n",
    "\n",
    "xmin_ST=xy_mds_ST[:,0].min() \n",
    "xmax_ST=xy_mds_ST[:,0].max()\n",
    "ymin_ST=xy_mds_ST[:,1].min()\n",
    "ymax_ST=xy_mds_ST[:,1].max()\n",
    "print(xmin_ST,xmax_ST,ymin_ST,ymax_ST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot spatial distribution on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lon_range = (-130.60, -52.75)\n",
    "lat_range = (17.13, 53.65)\n",
    "m = folium.Map(tiles='cartodbpositron', width='100%', height='100%') \n",
    "# If you adjusted the notebook display width to be as wide as your screen, the map might get very big. \n",
    "#Adjust size as desired.\n",
    "m.fit_bounds([[lat_range[0], lon_range[0]], [lat_range[1], lon_range[1]]])\n",
    "for id, row in acc_events_201301.iterrows():\n",
    "    cluster_id = row[clust_id_col_name]\n",
    "    if cluster_id != -1 and len(np.where(clusters_data.index==cluster_id)[0])>0:\n",
    "        i=np.where(clusters_data.index==cluster_id)[0][0]\n",
    "        if i<len(xy_mds_ST):\n",
    "            folium.CircleMarker((row['Latitude'], row['Longitude']), radius=2, \n",
    "                        #color=clust_colors[cluster_id % len(clust_colors)], \n",
    "                        color=getColor(xy_mds_ST[i,0], xy_mds_ST[i,1],xmin_ST,xmax_ST,ymin_ST,ymax_ST),\n",
    "                        fill=False, opacity=.3,\n",
    "                        popup='Cluster: {}'.format(cluster_id)).add_to(m)\n",
    "            \n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Clustering of UK accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_evt_hsall = df[\n",
    "             #(df['Local_Authority_(District)'].isin(london_borough)) &\n",
    "             (df.Accident_Severity !='Slight') &\n",
    "             #(df.Number_of_Casualties > 1) &\n",
    "             (df.Year >= 2017)]\n",
    "acc_evt_hsall.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hsall  = acc_evt_hsall[['Longitude','Latitude']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knee Locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=7)\n",
    "neighbors = nearest_neighbors.fit(X_hsall)\n",
    "distances, indices = neighbors.kneighbors(X_hsall)\n",
    "distances = np.sort(distances[:,6], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "i = np.arange(len(distances))\n",
    "knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "knee.plot_knee()\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "print(distances[knee.knee])\n",
    "#plt.savefig(\"Distance_curve.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbours = 7\n",
    "\n",
    "# represent points consistently as (lat, lon)\n",
    "coords = acc_evt_hsall[['Longitude','Latitude']]\n",
    "\n",
    "\n",
    "# define epsilon as 1.5 kilometers, converted to radians for use by haversine\n",
    "spatial_dist_max = distances[knee.knee] #/ kms_per_radian\n",
    "#spatial_dist_max = 0.00006\n",
    "\n",
    "clustered_ST = DBSCAN(eps=np.radians(distances[knee.knee]),\n",
    "                      metric='haversine',min_samples=n_neighbours).fit(np.radians(coords))\n",
    "\n",
    "print(\"Clustering finished!\")\n",
    "\n",
    "labels=clustered_ST.labels_\n",
    "unique_labels=np.unique(clustered_ST.labels_)\n",
    "print('Result: {} records in the noise, labelled as -1, and {} clusters labelled as 0..{}'.\n",
    "      format(acc_evt_hsall[labels==-1].shape[0], len(unique_labels)-1, len(unique_labels)-2))\n",
    "\n",
    "#clustered\n",
    "clust_id_col_name='ClusterN'\n",
    "acc_evt_hsall[clust_id_col_name]=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting cluster sizes\n",
    "cluster_sizes = acc_evt_hsall[clust_id_col_name].value_counts().rename_axis('Cluster id').to_frame('count')\n",
    "print(\"Cluster sizes:\")\n",
    "print(cluster_sizes.head(10))\n",
    "\n",
    "cluster_sizes = cluster_sizes[cluster_sizes.index != -1] # no noise\n",
    "\n",
    "max_cluster_size=cluster_sizes['count'].max()\n",
    "print(\"max = \",max_cluster_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_func = {\n",
    "    'DaysSince':['max','min'],\n",
    "    'Longitude':['mean','max','min'],\n",
    "    'Latitude':['mean','max','min'],\n",
    "    'DayYear':['max','min']\n",
    "}\n",
    "st_aggregates = acc_evt_hsall.reset_index(drop=False)[['ClusterN', 'DaysSince',\n",
    "                                                       'Longitude','Latitude', 'DayYear']].groupby(['ClusterN']).agg(agg_func)\n",
    "# Flatten hierarchical column names\n",
    "st_aggregates.columns = [\"_\".join(x) for x in st_aggregates.columns.ravel()]\n",
    "# compute derived attributes: duration and bounding rectangle diagonal\n",
    "st_aggregates['duration (days)']=st_aggregates['DaysSince_max']-st_aggregates['DaysSince_min']\n",
    "for id,row in st_aggregates.iterrows():\n",
    "    brd=kms_per_radian*great_circle2(row['Latitude_max'],row['Longitude_max'],\n",
    "                                     row['Latitude_min'],row['Longitude_min'])\n",
    "    #print('{}'.format(brd))\n",
    "    #print(row['Latitude_max'],row['Longitude_max'],row['Latitude_min'],row['Longitude_min'])\n",
    "    st_aggregates.at[id,'Bound_rect_diag(km)']=brd\n",
    "\n",
    "    \n",
    "\n",
    "clusters_data = st_aggregates.loc[st_aggregates.index!=-1,\n",
    "                                  ['Latitude_mean','Longitude_mean',\n",
    "                                   'DayYear_min','DayYear_max']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "clusters_data_scaled = scaler.fit_transform(clusters_data)\n",
    "\n",
    "mds_ST = MDS(n_components = 2, random_state=110)\n",
    "mds_ST.fit(clusters_data_scaled)\n",
    "xy_mds_ST = mds_ST.fit_transform(clusters_data_scaled)\n",
    "\n",
    "xmin_ST=xy_mds_ST[:,0].min() \n",
    "xmax_ST=xy_mds_ST[:,0].max()\n",
    "ymin_ST=xy_mds_ST[:,1].min()\n",
    "ymax_ST=xy_mds_ST[:,1].max()\n",
    "#print(xmin_ST,xmax_ST,ymin_ST,ymax_ST)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(18,16))\n",
    "\n",
    "plt.style.use(('fivethirtyeight'))\n",
    "\n",
    "plt.xlabel('Axis 1')\n",
    "plt.ylabel('Axis 2')\n",
    "plt.title('MDS projection (DBSCAN)')\n",
    "colors = [(0,0,0)]\n",
    "\n",
    "for i in range(len(xy_mds_ST)):\n",
    "    j=np.where(cluster_sizes.index==clusters_data.index[i])[0][0]\n",
    "    r=cluster_sizes.iat[j,0]/max_cluster_size\n",
    "    size=50 + 300*r\n",
    "    if r > 0.01:\n",
    "        cl_text = str(clusters_data.index[i])+\": \"+str(cluster_sizes.iat[j,0])\n",
    "    else:\n",
    "        cl_text = ''\n",
    "    ax.scatter(xy_mds_ST[i,0], xy_mds_ST[i,1], alpha = 1, s = size*2, \n",
    "                c=getColor(xy_mds_ST[i,0], xy_mds_ST[i,1],xmin_ST,xmax_ST,ymin_ST,ymax_ST))\n",
    "    ax.text(xy_mds_ST[i,0]+0.0001*size, xy_mds_ST[i,1]+0.0001*size,\n",
    "             cl_text, alpha = .6+.4*r, fontsize=18)\n",
    "\n",
    "        \n",
    "        \n",
    "#ax.set_facecolor('white')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#fig.savefig('mds_uk_hs_all_2dbscan.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tile = basemap_to_tiles(basemaps.Esri.WorldStreetMap)\n",
    "\n",
    "\n",
    "Esri_WorldStreetMap = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer/tile/{z}/{y}/{x}'\n",
    "Esri_Attribution = 'Tiles &copy; Esri &mdash; Source: Esri, DeLorme, NAVTEQ, USGS, Intermap, iPC, NRCAN, Esri Japan, METI, Esri China (Hong Kong), Esri (Thailand), TomTom, 2012'\n",
    "\n",
    "#location=[54.384797 , -3.438721],zoom_start=6)\n",
    "#location=[52.384797 , -3.438721],zoom_start=7)\n",
    "\n",
    "m = folium.Map(tiles=Esri_WorldStreetMap, attr=Esri_Attribution, width='100%', height='100%', \n",
    "               location=[52.384797 , -3.438721],zoom_start=7)\n",
    "# If you adjusted the notebook display width to be as wide as your screen, the map might get very big. \n",
    "#Adjust size as desired.\n",
    "#m.fit_bounds([[lat_range[0], lon_range[0]], [lat_range[1], lon_range[1]]])\n",
    "for id, row in acc_evt_hsall.iterrows():\n",
    "    cluster_id = row[clust_id_col_name]\n",
    "    if cluster_id != -1 and len(np.where(clusters_data.index==cluster_id)[0])>0:\n",
    "        i=np.where(clusters_data.index==cluster_id)[0][0]\n",
    "        if i<len(xy_mds_ST):\n",
    "            folium.CircleMarker((row['Latitude'], row['Longitude']), radius=2, \n",
    "                        #color=clust_colors[cluster_id % len(clust_colors)], \n",
    "                        color=getColor(xy_mds_ST[i,0], xy_mds_ST[i,1],xmin_ST,xmax_ST,ymin_ST,ymax_ST),\n",
    "                        fill=False, opacity=0.4,\n",
    "                        popup='Cluster: {}'.format(cluster_id)).add_to(m)\n",
    "            \n",
    "\n",
    "m\n",
    "\n",
    "#m.save(os.path.join('', 'all_accidents_lon.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the map as an HTML file\n",
    "\n",
    "delay=5\n",
    "\n",
    "fn='testmap.html'\n",
    "tmpurl='file://{path}/{mapfile}'.format(path=os.getcwd(),mapfile=fn)\n",
    "m.save(fn)\n",
    "\n",
    "#Open a browser window...\n",
    "browser = webdriver.Chrome()\n",
    "#..that displays the map...\n",
    "browser.get(tmpurl)\n",
    "#Give the map tiles some time to load\n",
    "time.sleep(delay)\n",
    "#Grab the screenshot\n",
    "browser.save_screenshot('map_uk_hs_dbscan_06a.png')\n",
    "#Close the browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of Histograms of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, row in acc_evt_hsall.iterrows():\n",
    "    cluster_id = row[clust_id_col_name]\n",
    "    if cluster_id != -1 and len(np.where(clusters_data.index==cluster_id)[0])>0:\n",
    "        i=np.where(clusters_data.index==cluster_id)[0][0]\n",
    "        if i<len(xy_mds_ST):\n",
    "            acc_evt_hsall.loc[id, 'Color']=getColor(xy_mds_ST[i,0], xy_mds_ST[i,1],\n",
    "                                                xmin_ST,xmax_ST,ymin_ST,ymax_ST)\n",
    "\n",
    "for id, row in acc_evt_hsmain.iterrows():\n",
    "    cluster_id = row[clust_id_col_name]\n",
    "    if cluster_id != -1 and len(np.where(clusters_data_main.index==cluster_id)[0])>0:\n",
    "        i=np.where(clusters_data_main.index==cluster_id)[0][0]\n",
    "        if i<len(xy_mds_ST_main):\n",
    "            acc_evt_hsmain.loc[id, 'Color']=getColor(xy_mds_ST_main[i,0], xy_mds_ST_main[i,1],\n",
    "                                                xmin_ST_main,xmax_ST_main,ymin_ST_main,ymax_ST_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_evt_hsall[acc_evt_hsall.ClusterN==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of UK and main clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UK clusters\n",
    "clust_val = acc_evt_hsall[acc_evt_hsall.ClusterN!=-1].groupby(['ClusterN'])['Accident_Index'].count(\n",
    ").reset_index().nlargest(20,'Accident_Index')['ClusterN'].values\n",
    "\n",
    "acc = acc_evt_hsall[acc_evt_hsall.ClusterN.isin(clust_val)].groupby(['ClusterN',\n",
    "               'Local_Authority_(District)'])['Accident_Index'].count().reset_index()\n",
    "\n",
    "acc_bar = acc.loc[acc.groupby(['ClusterN','color'])['Accident_Index'].idxmax()].reset_index()\n",
    "acc_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UK clusters\n",
    "clust_val = acc_evt_hsall[acc_evt_hsall.ClusterN!=-1].groupby(['ClusterN'])['Accident_Index'].count(\n",
    ").reset_index().nlargest(15,'Accident_Index')['ClusterN'].values\n",
    "\n",
    "acc = acc_evt_hsall[acc_evt_hsall.ClusterN.isin(clust_val)].groupby(['ClusterN',\n",
    "               'Local_Authority_(District)','Color'])['Accident_Index'].count().reset_index()\n",
    "\n",
    "acc_bar = acc.loc[acc.groupby(['ClusterN','Color'])['Accident_Index'].idxmax()].reset_index()\n",
    "\n",
    "# UK main cluster\n",
    "clust_val = acc_evt_hsmain[acc_evt_hsmain.ClusterN!=-1].groupby(['ClusterN'])['Accident_Index'].count(\n",
    ").reset_index().nlargest(15,'Accident_Index')['ClusterN'].values\n",
    "\n",
    "acc = acc_evt_hsmain[acc_evt_hsmain.ClusterN.isin(clust_val)].groupby(['ClusterN',\n",
    "               'Local_Authority_(District)','Color'])['Accident_Index'].count().reset_index()\n",
    "\n",
    "acc_main_bar = acc.loc[acc.groupby(['ClusterN'])['Accident_Index'].idxmax()].reset_index()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(32,13), sharey=True)\n",
    "\n",
    "clrs = acc_bar.Color\n",
    "clrs_main = acc_main_bar.Color\n",
    "# Barplot\n",
    "sns.barplot(data=acc_bar, x='Local_Authority_(District)', y='Accident_Index', palette=clrs, ax=axs[0])\n",
    "axs[0].set_xticklabels(axs[0].get_xticklabels(),rotation=90,fontsize=22)\n",
    "axs[0].yaxis.set_tick_params(labelsize=22)\n",
    "axs[0].set_title('Most frequent LA of each cluster (top 15 in size) in the UK',fontsize=22)\n",
    "axs[0].set_xlabel('')\n",
    "axs[0].set_ylabel('')\n",
    "\n",
    "sns.barplot(data=acc_main_bar, x='Local_Authority_(District)', y='Accident_Index', palette=clrs_main, ax=axs[1])\n",
    "axs[1].set_xticklabels(axs[1].get_xticklabels(),rotation=90,fontsize=22)\n",
    "axs[1].set_title('Most frequent LA of each cluster (top 15 in size) from main UK cluster',fontsize=22)\n",
    "axs[1].set_xlabel('')\n",
    "axs[1].set_ylabel('')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('fig12.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(acc_bar.Accident_Index.sum())\n",
    "display(acc_main_bar.Accident_Index.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df[df.Accident_Severity!='Slight'].copy()\n",
    "\n",
    "df3['Region'] = np.where(df3['Region']=='London','London',\n",
    "                                             df3['Region'])\n",
    "\n",
    "df3 = df3[df3['Region']!='London']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3[df3['Region']=='London'].count()/df3[df3['Region']!='London'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UK clusters\n",
    "# clust_val = acc_evt_hsall[acc_evt_hsall.ClusterN!=-1].groupby(['ClusterN'])['Accident_Index'].count(\n",
    "# ).reset_index().nlargest(15,'Accident_Index')['ClusterN'].values\n",
    "\n",
    "# acc = acc_evt_hsall[acc_evt_hsall.ClusterN.isin(clust_val)].groupby(['ClusterN',\n",
    "#                'Local_Authority_(District)','Color'])['Accident_Index'].count().reset_index()\n",
    "\n",
    "acc_2010 = df3[df3.Year==2010].groupby(['Region'])['Accident_Index'].count().reset_index().nlargest(10,'Accident_Index')\n",
    "acc_2011 = df3[df3.Year==2011].groupby(['Region'])['Accident_Index'].count().reset_index().nlargest(10,'Accident_Index')\n",
    "acc_2012 = df3[df3.Year==2012].groupby(['Region'])['Accident_Index'].count().reset_index().nlargest(10,'Accident_Index')\n",
    "acc_2013 = df3[df3.Year==2013].groupby(['Region'])['Accident_Index'].count().reset_index().nlargest(10,'Accident_Index')\n",
    "acc_2014 = df3[df3.Year==2014].groupby(['Region'])['Accident_Index'].count().reset_index().nlargest(10,'Accident_Index')\n",
    "acc_2015 = df3[df3.Year==2015].groupby(['Region'])['Accident_Index'].count().reset_index().nlargest(10,'Accident_Index')\n",
    "acc_2016 = df3[df3.Year==2016].groupby(['Region'])['Accident_Index'].count().reset_index().nlargest(10,'Accident_Index')\n",
    "acc_2017 = df3[df3.Year==2017].groupby(['Region'])['Accident_Index'].count().reset_index().nlargest(10,'Accident_Index')\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(8,1,figsize=(32,38))\n",
    "\n",
    " \n",
    "# Barplot\n",
    "sns.barplot(data=acc_2010, x='Region', y='Accident_Index', ax=axs[0])\n",
    "sns.barplot(data=acc_2011, x='Region', y='Accident_Index', ax=axs[1])\n",
    "sns.barplot(data=acc_2012, x='Region', y='Accident_Index', ax=axs[2])\n",
    "sns.barplot(data=acc_2013, x='Region', y='Accident_Index', ax=axs[3])\n",
    "sns.barplot(data=acc_2014, x='Region', y='Accident_Index', ax=axs[4])\n",
    "sns.barplot(data=acc_2015, x='Region', y='Accident_Index', ax=axs[5])\n",
    "sns.barplot(data=acc_2016, x='Region', y='Accident_Index', ax=axs[6])\n",
    "sns.barplot(data=acc_2017, x='Region', y='Accident_Index', ax=axs[7])\n",
    "axs[0].set_xticklabels(axs[0].get_xticklabels(),rotation=90,fontsize=18)\n",
    "axs[1].set_xticklabels(axs[1].get_xticklabels(),rotation=90,fontsize=18)\n",
    "axs[2].set_xticklabels(axs[2].get_xticklabels(),rotation=90,fontsize=18)\n",
    "axs[3].set_xticklabels(axs[3].get_xticklabels(),rotation=90,fontsize=18)\n",
    "axs[4].set_xticklabels(axs[4].get_xticklabels(),rotation=90,fontsize=18)\n",
    "axs[5].set_xticklabels(axs[5].get_xticklabels(),rotation=90,fontsize=18)\n",
    "axs[6].set_xticklabels(axs[6].get_xticklabels(),rotation=90,fontsize=18)\n",
    "axs[7].set_xticklabels(axs[7].get_xticklabels(),rotation=90,fontsize=18)\n",
    "\n",
    "# axs[0].yaxis.set_tick_params(labelsize=22)\n",
    "# axs[0].set_title('Most frequent LA of each cluster (top 15 in size) in the UK',fontsize=22)\n",
    "# axs[0].set_xlabel('')\n",
    "# axs[0].set_ylabel('')\n",
    "\n",
    "# sns.barplot(data=acc_main_bar, x='Local_Authority_(District)', y='Accident_Index', palette=clrs_main, ax=axs[1])\n",
    "# axs[1].set_xticklabels(axs[1].get_xticklabels(),rotation=90,fontsize=22)\n",
    "# axs[1].set_title('Most frequent LA of each cluster (top 15 in size) from main UK cluster',fontsize=22)\n",
    "# axs[1].set_xlabel('')\n",
    "# axs[1].set_ylabel('')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of UK main cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_evt_hsmain = acc_evt_hsall[acc_evt_hsall.ClusterN==0]\n",
    "X_hsmain = acc_evt_hsall[acc_evt_hsall.ClusterN==0][['Longitude','Latitude']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_hsall  = acc_evt_hsall[['Longitude','Latitude']].values\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=10)\n",
    "neighbors = nearest_neighbors.fit(X_hsmain)\n",
    "distances, indices = neighbors.kneighbors(X_hsmain)\n",
    "distances = np.sort(distances[:,9], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "i = np.arange(len(distances))\n",
    "knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n",
    "# fig = plt.figure(figsize=(5, 5))\n",
    "# knee.plot_knee()\n",
    "# plt.xlabel(\"Points\")\n",
    "# plt.ylabel(\"Distance\")\n",
    "\n",
    "print(distances[knee.knee])\n",
    "#plt.savefig(\"Distance_curve.png\", dpi=300)\n",
    "\n",
    "n_neighbours = 10\n",
    "\n",
    "# represent points consistently as (lat, lon)\n",
    "coords = acc_evt_hsmain[['Longitude','Latitude']]\n",
    "\n",
    "\n",
    "# define epsilon as 1.5 kilometers, converted to radians for use by haversine\n",
    "spatial_dist_max = distances[knee.knee] #/ kms_per_radian\n",
    "#spatial_dist_max = 0.00006\n",
    "\n",
    "clustered_ST = DBSCAN(eps=np.radians(distances[knee.knee]),\n",
    "                      metric='haversine',min_samples=n_neighbours).fit(np.radians(coords))\n",
    "\n",
    "print(\"Clustering finished!\")\n",
    "\n",
    "labels=clustered_ST.labels_\n",
    "unique_labels=np.unique(clustered_ST.labels_)\n",
    "print('Result: {} records in the noise, labelled as -1, and {} clusters labelled as 0..{}'.\n",
    "      format(acc_evt_hsmain[labels==-1].shape[0], len(unique_labels)-1, len(unique_labels)-2))\n",
    "\n",
    "#clustered\n",
    "clust_id_col_name='ClusterN'\n",
    "acc_evt_hsmain[clust_id_col_name]=labels\n",
    "\n",
    "## Getting cluster sizes\n",
    "cluster_sizes = acc_evt_hsmain[clust_id_col_name].value_counts().rename_axis('Cluster id').to_frame('count')\n",
    "print(\"Cluster sizes:\")\n",
    "print(cluster_sizes.head(10))\n",
    "\n",
    "cluster_sizes = cluster_sizes[cluster_sizes.index != -1] # no noise\n",
    "\n",
    "max_cluster_size=cluster_sizes['count'].max()\n",
    "print(\"max = \",max_cluster_size)\n",
    "\n",
    "agg_func = {\n",
    "    'DaysSince':['max','min'],\n",
    "    'Longitude':['mean','max','min'],\n",
    "    'Latitude':['mean','max','min'],\n",
    "    'DayYear':['max','min']\n",
    "}\n",
    "st_aggregates = acc_evt_hsmain.reset_index(drop=False)[['ClusterN', 'DaysSince',\n",
    "                                                       'Longitude','Latitude', 'DayYear']].groupby(['ClusterN']).agg(agg_func)\n",
    "# Flatten hierarchical column names\n",
    "st_aggregates.columns = [\"_\".join(x) for x in st_aggregates.columns.ravel()]\n",
    "# compute derived attributes: duration and bounding rectangle diagonal\n",
    "st_aggregates['duration (days)']=st_aggregates['DaysSince_max']-st_aggregates['DaysSince_min']\n",
    "for id,row in st_aggregates.iterrows():\n",
    "    brd=kms_per_radian*great_circle2(row['Latitude_max'],row['Longitude_max'],\n",
    "                                     row['Latitude_min'],row['Longitude_min'])\n",
    "    #print('{}'.format(brd))\n",
    "    #print(row['Latitude_max'],row['Longitude_max'],row['Latitude_min'],row['Longitude_min'])\n",
    "    st_aggregates.at[id,'Bound_rect_diag(km)']=brd\n",
    "\n",
    "    \n",
    "\n",
    "clusters_data_main = st_aggregates.loc[st_aggregates.index!=-1,\n",
    "                                  ['Latitude_mean','Longitude_mean',\n",
    "                                   'DayYear_min','DayYear_max']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "clusters_data_scaled = scaler.fit_transform(clusters_data_main)\n",
    "\n",
    "mds_ST = MDS(n_components = 2, random_state=110)\n",
    "mds_ST.fit(clusters_data_scaled)\n",
    "\n",
    "xy_mds_ST_main = mds_ST.fit_transform(clusters_data_scaled)\n",
    "\n",
    "xmin_ST_main=xy_mds_ST_main[:,0].min() \n",
    "xmax_ST_main=xy_mds_ST_main[:,0].max()\n",
    "ymin_ST_main=xy_mds_ST_main[:,1].min()\n",
    "ymax_ST_main=xy_mds_ST_main[:,1].max()\n",
    "#print(xmin_ST,xmax_ST,ymin_ST,ymax_ST)\n",
    "\n",
    "\n",
    "\n",
    "tile = basemap_to_tiles(basemaps.Esri.WorldStreetMap)\n",
    "\n",
    "\n",
    "Esri_WorldStreetMap = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer/tile/{z}/{y}/{x}'\n",
    "Esri_Attribution = 'Tiles &copy; Esri &mdash; Source: Esri, DeLorme, NAVTEQ, USGS, Intermap, iPC, NRCAN, Esri Japan, METI, Esri China (Hong Kong), Esri (Thailand), TomTom, 2012'\n",
    "\n",
    "#location=[54.384797 , -3.438721],zoom_start=6)\n",
    "#location=[52.384797 , -3.438721],zoom_start=7)\n",
    "\n",
    "m = folium.Map(tiles=Esri_WorldStreetMap, attr=Esri_Attribution, width='100%', height='100%', \n",
    "               location=[52.384797 , -3.438721],zoom_start=7)\n",
    "# If you adjusted the notebook display width to be as wide as your screen, the map might get very big. \n",
    "#Adjust size as desired.\n",
    "#m.fit_bounds([[lat_range[0], lon_range[0]], [lat_range[1], lon_range[1]]])\n",
    "for id, row in acc_evt_hsmain.iterrows():\n",
    "    cluster_id = row[clust_id_col_name]\n",
    "    if cluster_id != -1 and len(np.where(clusters_data.index==cluster_id)[0])>0:\n",
    "        i=np.where(clusters_data.index==cluster_id)[0][0]\n",
    "        if i<len(xy_mds_ST_main):\n",
    "            folium.CircleMarker((row['Latitude'], row['Longitude']), radius=2, \n",
    "                        #color=clust_colors[cluster_id % len(clust_colors)], \n",
    "                        color=getColor(xy_mds_ST_main[i,0], xy_mds_ST_main[i,1],\n",
    "                                       xmin_ST_main,xmax_ST_main,ymin_ST_main,ymax_ST_main),\n",
    "                        fill=False, opacity=0.4,\n",
    "                        popup='Cluster: {}'.format(cluster_id)).add_to(m)\n",
    "            \n",
    "\n",
    "m\n",
    "\n",
    "\n",
    "# delay=5\n",
    "\n",
    "# fn='testmap.html'\n",
    "# tmpurl='file://{path}/{mapfile}'.format(path=os.getcwd(),mapfile=fn)\n",
    "# m.save(fn)\n",
    "\n",
    "# #Open a browser window...\n",
    "# browser = webdriver.Chrome()\n",
    "# #..that displays the map...\n",
    "# browser.get(tmpurl)\n",
    "# #Give the map tiles some time to load\n",
    "# time.sleep(delay)\n",
    "# #Grab the screenshot\n",
    "# browser.save_screenshot('map_ukclus1_hs_dbscan.png')\n",
    "# #Close the browser\n",
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_val = acc_evt_hsmain[acc_evt_hsmain.ClusterN!=-1].groupby(['ClusterN'])['Accident_Index'].count(\n",
    ").reset_index().nlargest(20,'Accident_Index')['ClusterN'].values\n",
    "\n",
    "acc = acc_evt_hsmain[acc_evt_hsmain.ClusterN.isin(clust_val)].groupby(['ClusterN',\n",
    "               'Local_Authority_(District)'])['Accident_Index'].count().reset_index()\n",
    "\n",
    "acc.loc[acc.groupby(['ClusterN'])['Accident_Index'].idxmax()]['Accident_Index'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_val = acc_evt_hsall[acc_evt_hsall.ClusterN!=-1].groupby(['ClusterN'])['Accident_Index'].count(\n",
    ").reset_index().nlargest(20,'Accident_Index')['ClusterN'].values\n",
    "\n",
    "acc = acc_evt_hsall[acc_evt_hsall.ClusterN.isin(clust_val)].groupby(['ClusterN',\n",
    "               'Local_Authority_(District)'])['Accident_Index'].count().reset_index()\n",
    "\n",
    "acc.loc[acc.groupby(['ClusterN'])['Accident_Index'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Clustering of London accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_borough = ['Hackney',\n",
    "'Barking and Dagenham','Barnet',\n",
    "'Camden',\n",
    "'Hillingdon',\n",
    "'Brent',\n",
    "'Harrow',\n",
    "'Croydon',\n",
    "'Bexley',\n",
    "'Ealing',\n",
    "'Enfield',\n",
    "'Hounslow',\n",
    "'Hammersmith and Fulham',\n",
    "'Havering',\n",
    "'Bromley',\n",
    "'Haringey',\n",
    "'Islington',\n",
    "'Kensington and Chelsea',\n",
    "'City of London',\n",
    "'Greenwich',\n",
    "'Westminster',\n",
    "'Lewisham',\n",
    "'Southwark',\n",
    "'Newham',\n",
    "'Tower Hamlets',\n",
    "'Redbridge',\n",
    "'Sutton',\n",
    "'Waltham Forest',\n",
    "'Merton',\n",
    "'Richmond upon Thames',\n",
    "'Kingston upon Thames',\n",
    "'Lambeth',\n",
    "'Wandsworth']\n",
    "\n",
    "\n",
    "#acc_london = acc_events[(acc_events.Year>=2013)] \n",
    "acc_evt_hs = df[\n",
    "             #(df['Local_Authority_(District)'].isin(london_borough)) &\n",
    "             (df.Region == 'London') &\n",
    "             (df.Accident_Severity !='Slight') &\n",
    "             #(df.Number_of_Casualties > 1) &\n",
    "             (df.Year >= 2017)]\n",
    "\n",
    "acc_evt_ls = df[\n",
    "             #(df['Local_Authority_(District)'].isin(london_borough)) &\n",
    "             (df.Region == 'London') &\n",
    "             (df.Accident_Severity =='Slight') &\n",
    "             #(df.Number_of_Casualties > 1) &\n",
    "             (df.Year >= 2015)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like first to analyse what would be the best choice for the parameter epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LS:',acc_evt_ls.shape)\n",
    "print('HS:',acc_evt_hs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_hs[:,0].min(),X_hs[:,0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = acc_lon[['Longitude','Latitude']].values\n",
    "X_hs  = acc_evt_hs[['Longitude','Latitude']].values\n",
    "X_ls  = acc_evt_ls[['Longitude','Latitude']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High severity accidents DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NearestNeighbors locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=9)\n",
    "neighbors = nearest_neighbors.fit(X_hs)\n",
    "distances, indices = neighbors.kneighbors(X_hs)\n",
    "distances = np.sort(distances[:,8], axis=0)\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.plot(distances)\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Distance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knee locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "i = np.arange(len(distances))\n",
    "knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "knee.plot_knee()\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "print(distances[knee.knee])\n",
    "plt.savefig(\"Distance_curve.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot using EPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(15, 10))\n",
    "# fig.subplots_adjust(hspace=.5, wspace=.2)\n",
    "# i = 1\n",
    "\n",
    "# # represent points consistently as (lat, lon)\n",
    "# coords = acc_eng[['Longitude','Latitude']][:100].values\n",
    "\n",
    "# X=coords\n",
    "\n",
    "# for x in range(10, 0, -1):\n",
    "#     eps = 1/(10*(11-x))\n",
    "#     db = DBSCAN(eps=eps, min_samples=7).fit(X)\n",
    "#     core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "#     core_samples_mask[db.core_sample_indices_] = True\n",
    "#     labels = db.labels_\n",
    "    \n",
    "#     #print(eps)\n",
    "#     ax = fig.add_subplot(2, 5, i)\n",
    "#     #ax.text(1, 4, \"eps = {}\".format(round(eps, 3)), fontsize=25, ha=\"center\")\n",
    "#     sns.scatterplot(X[:,0], X[:,1], hue=[\"cluster-{}\".format(x) for x in labels])\n",
    "#     i += 1\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temporal_dist_max = 24\n",
    "n_neighbours = 9\n",
    "\n",
    "# represent points consistently as (lat, lon)\n",
    "coords = acc_evt_hs[['Longitude','Latitude']]\n",
    "\n",
    "\n",
    "# define epsilon as 1.5 kilometers, converted to radians for use by haversine\n",
    "spatial_dist_max = distances[knee.knee] #/ kms_per_radian\n",
    "#spatial_dist_max = 0.00006\n",
    "#distances[knee.knee]\n",
    "clustered_ST = DBSCAN(eps=np.radians(spatial_dist_max),\n",
    "                      metric='haversine',min_samples=n_neighbours).fit(np.radians(coords))\n",
    "\n",
    "print(\"Clustering finished!\")\n",
    "\n",
    "labels=clustered_ST.labels_\n",
    "unique_labels=np.unique(clustered_ST.labels_)\n",
    "print('Result: {} records in the noise, labelled as -1, and {} clusters labelled as 0..{}'.\n",
    "      format(acc_evt_hs[labels==-1].shape[0], len(unique_labels)-1, len(unique_labels)-2))\n",
    "\n",
    "#clustered\n",
    "clust_id_col_name='ClusterN'\n",
    "acc_evt_hs[clust_id_col_name]=labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# days_week = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# # Scale X\n",
    "# X_hs = acc_evt_hs[['Longitude','Latitude','Hour', 'Number_of_Casualties']][:500].values\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X_hs)\n",
    "\n",
    "# mds = MDS(2,random_state=0)\n",
    "# X_2d = mds.fit_transform(X_scaled)\n",
    "\n",
    "# colors = ['red','yellow','green','blue','purple','orange','brown']\n",
    "# plt.rcParams['figure.figsize'] = [12, 10]\n",
    "# plt.rc('font', size=14)\n",
    "\n",
    "# my_cmap = plt.get_cmap('BuPu')\n",
    "\n",
    "# k=1\n",
    "# for i in days_week:\n",
    "    \n",
    "#     subset = X_2d[np.array(acc_evt_hs[:500].Day_of_Week) == i]\n",
    "  \n",
    "#     x = [row[0] for row in subset]\n",
    "#     y = [row[1] for row in subset]\n",
    "#     plt.scatter(x,y,\n",
    "#                 color=my_cmap(k / 7),label=i,alpha=0.7)\n",
    "#     k+=1\n",
    "#     #Day_of_Week\n",
    "    \n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# #fig.savefig('MDS HS - Latitude, Longitude, Hour and Number of Casualties.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### London Cluster (DBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Getting cluster sizes\n",
    "cluster_sizes = acc_evt_hs[clust_id_col_name].value_counts().rename_axis('Cluster id').to_frame('count')\n",
    "print(\"Cluster sizes:\")\n",
    "print(cluster_sizes.head(10))\n",
    "\n",
    "cluster_sizes = cluster_sizes[cluster_sizes.index != -1] # no noise\n",
    "\n",
    "max_cluster_size=cluster_sizes['count'].max()\n",
    "print(\"max = \",max_cluster_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_func = {\n",
    "    'DaysSince':['max','min'],\n",
    "    'Longitude':['mean','max','min'],\n",
    "    'Latitude':['mean','max','min']\n",
    "}\n",
    "st_aggregates = acc_evt_hs.reset_index(drop=False)[['ClusterN','DaysSince',\n",
    "                                                'Longitude','Latitude']].groupby(['ClusterN']).agg(agg_func)\n",
    "# Flatten hierarchical column names\n",
    "st_aggregates.columns = [\"_\".join(x) for x in st_aggregates.columns.ravel()]\n",
    "# compute derived attributes: duration and bounding rectangle diagonal\n",
    "st_aggregates['duration (days)']=st_aggregates['DaysSince_max']-st_aggregates['DaysSince_min']\n",
    "for id,row in st_aggregates.iterrows():\n",
    "    brd=kms_per_radian*great_circle2(row['Latitude_max'],row['Longitude_max'],\n",
    "                                     row['Latitude_min'],row['Longitude_min'])\n",
    "    #print('{}'.format(brd))\n",
    "    #print(row['Latitude_max'],row['Longitude_max'],row['Latitude_min'],row['Longitude_min'])\n",
    "    st_aggregates.at[id,'Bound_rect_diag(km)']=brd\n",
    "\n",
    "    \n",
    "\n",
    "clusters_data = st_aggregates.loc[st_aggregates.index!=-1,\n",
    "                                  ['Latitude_mean','Longitude_mean',\n",
    "                                   'DaysSince_min','DaysSince_max']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "clusters_data_scaled = scaler.fit_transform(clusters_data)\n",
    "\n",
    "mds_ST = MDS(n_components = 2, random_state=110)\n",
    "mds_ST.fit(clusters_data_scaled)\n",
    "xy_mds_ST = mds_ST.fit_transform(clusters_data_scaled)\n",
    "\n",
    "xmin_ST=xy_mds_ST[:,0].min() \n",
    "xmax_ST=xy_mds_ST[:,0].max()\n",
    "ymin_ST=xy_mds_ST[:,1].min()\n",
    "ymax_ST=xy_mds_ST[:,1].max()\n",
    "#print(xmin_ST,xmax_ST,ymin_ST,ymax_ST)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "plt.xlabel('Axis 1')\n",
    "plt.ylabel('Axis 2')\n",
    "plt.title('MDS projection (DBSCAN)')\n",
    "colors = [(0,0,0)]\n",
    "for i in range(len(xy_mds_ST)):\n",
    "    j=np.where(cluster_sizes.index==clusters_data.index[i])[0][0]\n",
    "    r=cluster_sizes.iat[j,0]/max_cluster_size\n",
    "    size=50 + 300*r\n",
    "    ax.scatter(xy_mds_ST[i,0], xy_mds_ST[i,1], alpha = .9, s = size, \n",
    "                c=getColor(xy_mds_ST[i,0], xy_mds_ST[i,1],xmin_ST,xmax_ST,ymin_ST,ymax_ST))\n",
    "    ax.text(xy_mds_ST[i,0]+0.0001*size, xy_mds_ST[i,1]+0.0001*size,\n",
    "             str(clusters_data.index[i])+\": \"+str(cluster_sizes.iat[j,0]), alpha = .6+.4*r)\n",
    "\n",
    "plt.grid() \n",
    "plt.show()\n",
    "\n",
    "#fig.savefig('mdslonhs_dbscan.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Plot (High severity accidents in London 2015-2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tile = basemap_to_tiles(basemaps.Esri.WorldStreetMap)\n",
    "\n",
    "\n",
    "# Esri_WorldStreetMap = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer/tile/{z}/{y}/{x}'\n",
    "# Esri_Attribution = 'Tiles &copy; Esri &mdash; Source: Esri, DeLorme, NAVTEQ, USGS, Intermap, iPC, NRCAN, Esri Japan, METI, Esri China (Hong Kong), Esri (Thailand), TomTom, 2012'\n",
    "\n",
    "# lon_range = (-130.60, -52.75)\n",
    "# lat_range = (17.13, 53.65)\n",
    "# m = folium.Map(tiles=Esri_WorldStreetMap, attr=Esri_Attribution, width='100%', height='100%', \n",
    "#                location=[53.384797 , -3.438721],zoom_start=7) \n",
    "# # If you adjusted the notebook display width to be as wide as your screen, the map might get very big. \n",
    "# #Adjust size as desired.\n",
    "# #m.fit_bounds([[lat_range[0], lon_range[0]], [lat_range[1], lon_range[1]]])\n",
    "\n",
    "# folium.CircleMarker((acc_evt_hs['Latitude'], acc_evt_hs['Longitude']), radius=2, \n",
    "#             #color=clust_colors[cluster_id % len(clust_colors)], \n",
    "#             color='red', fill=False, opacity=.3).add_to(m)\n",
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tile = basemap_to_tiles(basemaps.Esri.WorldStreetMap)\n",
    "\n",
    "\n",
    "Esri_WorldStreetMap = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer/tile/{z}/{y}/{x}'\n",
    "Esri_Attribution = 'Tiles &copy; Esri &mdash; Source: Esri, DeLorme, NAVTEQ, USGS, Intermap, iPC, NRCAN, Esri Japan, METI, Esri China (Hong Kong), Esri (Thailand), TomTom, 2012'\n",
    "\n",
    "\n",
    "m = folium.Map(tiles=Esri_WorldStreetMap, attr=Esri_Attribution, width='100%', height='100%', \n",
    "               location=[51.504797 , -0.068721],zoom_start=10) \n",
    "# If you adjusted the notebook display width to be as wide as your screen, the map might get very big. \n",
    "#Adjust size as desired.\n",
    "#m.fit_bounds([[lat_range[0], lon_range[0]], [lat_range[1], lon_range[1]]])\n",
    "for id, row in acc_evt_hs.iterrows():\n",
    "    cluster_id = row[clust_id_col_name]\n",
    "    if cluster_id != -1 and len(np.where(clusters_data.index==cluster_id)[0])>0:\n",
    "        i=np.where(clusters_data.index==cluster_id)[0][0]\n",
    "        if i<len(xy_mds_ST):\n",
    "            folium.CircleMarker((row['Latitude'], row['Longitude']), radius=2, \n",
    "                        #color=clust_colors[cluster_id % len(clust_colors)], \n",
    "                        color=getColor(xy_mds_ST[i,0], xy_mds_ST[i,1],xmin_ST,xmax_ST,ymin_ST,ymax_ST),\n",
    "                        fill=False, opacity=1,\n",
    "                        popup='Cluster: {}'.format(cluster_id)).add_to(m)\n",
    "            \n",
    "\n",
    "m\n",
    "\n",
    "#m.save(os.path.join('', 'all_accidents_lon.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the map as an HTML file\n",
    "fn='testmap.html'\n",
    "tmpurl='file://{path}/{mapfile}'.format(path=os.getcwd(),mapfile=fn)\n",
    "m.save(fn)\n",
    "\n",
    "#Open a browser window...\n",
    "browser = webdriver.Chrome()\n",
    "#..that displays the map...\n",
    "browser.get(tmpurl)\n",
    "#Give the map tiles some time to load\n",
    "time.sleep(delay)\n",
    "#Grab the screenshot\n",
    "browser.save_screenshot('map_lon_hs_dbscan.png')\n",
    "#Close the browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the spatio-temporal distribution of the UK cluster members in a 3D view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(1, figsize=(24, 15))\n",
    "\n",
    "plt.style.use(('fivethirtyeight'))\n",
    "\n",
    "for id,row in acc_evt_hsall.iterrows():\n",
    "    cluster_id = row[clust_id_col_name]\n",
    "    if (cluster_id==-1) or len(np.where(clusters_data.index==cluster_id)[0])==0 :\n",
    "        color='#000000'\n",
    "    else:\n",
    "        i=np.where(clusters_data.index==cluster_id)[0][0]\n",
    "        color=getColor(xy_mds_ST[i,0], xy_mds_ST[i,1],xmin_ST,xmax_ST,ymin_ST,ymax_ST)\n",
    "        #color=clust_colors[cluster_id % len(clust_colors)]\n",
    "    acc_evt_hsall.at[id,'colors']=color\n",
    "acc_evt_hsallnn = acc_evt_hsall[acc_evt_hsall[clust_id_col_name] != -1] # no noise \n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=35, azim=-55) # change parameters here for experimenting\n",
    "\n",
    "fig.set_facecolor('white')\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "ax.scatter(acc_evt_hsallnn['Longitude'], acc_evt_hsallnn['Latitude'], acc_evt_hsallnn['DayYear'], \n",
    "           c=acc_evt_hsallnn['colors'], alpha=0.8)\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('axes3d_uk2_dbscan.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low severity accidents DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NearestNeighbors locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=7)\n",
    "neighbors = nearest_neighbors.fit(X_ls)\n",
    "distances, indices = neighbors.kneighbors(X_ls)\n",
    "distances = np.sort(distances[:,6], axis=0)\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.plot(distances)\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Distance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knee locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = np.arange(len(distances))\n",
    "knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "knee.plot_knee()\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "print(distances[knee.knee])\n",
    "plt.savefig(\"Distance_curve.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot using EPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(15, 10))\n",
    "# fig.subplots_adjust(hspace=.5, wspace=.2)\n",
    "# i = 1\n",
    "\n",
    "# # represent points consistently as (lat, lon)\n",
    "# coords = acc_eng[['Longitude','Latitude']][:100].values\n",
    "\n",
    "# X=coords\n",
    "\n",
    "# for x in range(10, 0, -1):\n",
    "#     eps = 1/(10*(11-x))\n",
    "#     db = DBSCAN(eps=eps, min_samples=7).fit(X)\n",
    "#     core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "#     core_samples_mask[db.core_sample_indices_] = True\n",
    "#     labels = db.labels_\n",
    "    \n",
    "#     #print(eps)\n",
    "#     ax = fig.add_subplot(2, 5, i)\n",
    "#     #ax.text(1, 4, \"eps = {}\".format(round(eps, 3)), fontsize=25, ha=\"center\")\n",
    "#     sns.scatterplot(X[:,0], X[:,1], hue=[\"cluster-{}\".format(x) for x in labels])\n",
    "#     i += 1\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_dist_max = 24\n",
    "n_neighbours = 7\n",
    "\n",
    "# represent points consistently as (lat, lon)\n",
    "coords = acc_evt_ls[['Longitude','Latitude']]\n",
    "\n",
    "\n",
    "# define epsilon as 1.5 kilometers, converted to radians for use by haversine\n",
    "spatial_dist_max = distances[knee.knee] #/ kms_per_radian\n",
    "#spatial_dist_max = 0.00006\n",
    "\n",
    "clustered_ST = DBSCAN(eps=np.radians(distances[knee.knee]),\n",
    "                      metric='haversine',min_samples=n_neighbours).fit(np.radians(coords))\n",
    "\n",
    "print(\"Clustering finished!\")\n",
    "\n",
    "labels=clustered_ST.labels_\n",
    "unique_labels=np.unique(clustered_ST.labels_)\n",
    "print('Result: {} records in the noise, labelled as -1, and {} clusters labelled as 0..{}'.\n",
    "      format(acc_evt_ls[labels==-1].shape[0], len(unique_labels)-1, len(unique_labels)-2))\n",
    "\n",
    "#clustered\n",
    "clust_id_col_name='ClusterN'\n",
    "acc_evt_ls[clust_id_col_name]=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting cluster sizes\n",
    "cluster_sizes = acc_evt_ls[clust_id_col_name].value_counts().rename_axis('Cluster id').to_frame('count')\n",
    "print(\"Cluster sizes:\")\n",
    "print(cluster_sizes.head(10))\n",
    "\n",
    "cluster_sizes = cluster_sizes[cluster_sizes.index != -1] # no noise\n",
    "\n",
    "max_cluster_size=cluster_sizes['count'].max()\n",
    "print(\"max = \",max_cluster_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_func = {\n",
    "    'DaysSince':['max','min'],\n",
    "    'Longitude':['mean','max','min'],\n",
    "    'Latitude':['mean','max','min']\n",
    "}\n",
    "st_aggregates = acc_evt_ls.reset_index(drop=False)[['ClusterN','DaysSince',\n",
    "                                                'Longitude','Latitude']].groupby(['ClusterN']).agg(agg_func)\n",
    "# Flatten hierarchical column names\n",
    "st_aggregates.columns = [\"_\".join(x) for x in st_aggregates.columns.ravel()]\n",
    "# compute derived attributes: duration and bounding rectangle diagonal\n",
    "st_aggregates['duration (days)']=st_aggregates['DaysSince_max']-st_aggregates['DaysSince_min']\n",
    "for id,row in st_aggregates.iterrows():\n",
    "    brd=kms_per_radian*great_circle2(row['Latitude_max'],row['Longitude_max'],row['Latitude_min'],row['Longitude_min'])\n",
    "    #print('{}'.format(brd))\n",
    "    #print(row['Latitude_max'],row['Longitude_max'],row['Latitude_min'],row['Longitude_min'])\n",
    "    st_aggregates.at[id,'Bound_rect_diag(km)']=brd\n",
    "\n",
    "    \n",
    "\n",
    "clusters_data = st_aggregates.loc[st_aggregates.index!=-1,\n",
    "                                  ['Latitude_mean','Longitude_mean',\n",
    "                                   'DaysSince_min','DaysSince_max']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "clusters_data_scaled = scaler.fit_transform(clusters_data)\n",
    "\n",
    "mds_ST = MDS(n_components = 2, random_state=110)\n",
    "mds_ST.fit(clusters_data_scaled)\n",
    "xy_mds_ST = mds_ST.fit_transform(clusters_data_scaled)\n",
    "\n",
    "xmin_ST=xy_mds_ST[:,0].min() \n",
    "xmax_ST=xy_mds_ST[:,0].max()\n",
    "ymin_ST=xy_mds_ST[:,1].min()\n",
    "ymax_ST=xy_mds_ST[:,1].max()\n",
    "#print(xmin_ST,xmax_ST,ymin_ST,ymax_ST)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "plt.xlabel('Axis 1')\n",
    "plt.ylabel('Axis 2')\n",
    "plt.title('MDS projection (DBSCAN)')\n",
    "colors = [(0,0,0)]\n",
    "for i in range(len(xy_mds_ST)):\n",
    "    j=np.where(cluster_sizes.index==clusters_data.index[i])[0][0]\n",
    "    r=cluster_sizes.iat[j,0]/max_cluster_size\n",
    "    size=50 + 300*r\n",
    "    ax.scatter(xy_mds_ST[i,0], xy_mds_ST[i,1], alpha = .9, s = size, \n",
    "                c=getColor(xy_mds_ST[i,0], xy_mds_ST[i,1],xmin_ST,xmax_ST,ymin_ST,ymax_ST))\n",
    "    ax.text(xy_mds_ST[i,0]+0.0001*size, xy_mds_ST[i,1]+0.0001*size,\n",
    "             str(clusters_data.index[i])+\": \"+str(cluster_sizes.iat[j,0]), alpha = .6+.4*r)\n",
    "\n",
    "plt.grid() \n",
    "plt.show()\n",
    "\n",
    "fig.savefig('mdslonls_dbscan.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot (All accidents in London 2015-2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plotDot(point):\n",
    "#     '''input: series that contains a numeric named latitude and a numeric named longitude\n",
    "#     this function creates a CircleMarker and adds it to your this_map'''\n",
    "#     htmlString = folium.Html(popopHTMLString(point), script=True)\n",
    "#     folium.CircleMarker(location=[point.latitude, point.longitude],\n",
    "#                         fill_color=color_dict[point[color_var]],   ####NOTE THE CHANGE IN THE COLOR\n",
    "#                         radius=2,\n",
    "#                         popup = folium.Popup(htmlString),\n",
    "#                         weight=0).add_to(this_map)\n",
    "\n",
    " \n",
    "    \n",
    "# #use df.apply(,axis=1) to iterate through every row in your dataframe\n",
    "# data.apply(plotDot, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tile = basemap_to_tiles(basemaps.Esri.WorldStreetMap)\n",
    "\n",
    "\n",
    "# Esri_WorldStreetMap = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer/tile/{z}/{y}/{x}'\n",
    "# Esri_Attribution = 'Tiles &copy; Esri &mdash; Source: Esri, DeLorme, NAVTEQ, USGS, Intermap, iPC, NRCAN, Esri Japan, METI, Esri China (Hong Kong), Esri (Thailand), TomTom, 2012'\n",
    "\n",
    "# lon_range = (-130.60, -52.75)\n",
    "# lat_range = (17.13, 53.65)\n",
    "# m = folium.Map(tiles=Esri_WorldStreetMap, attr=Esri_Attribution, width='100%', height='100%', \n",
    "#                location=[53.384797 , -3.438721],zoom_start=7) \n",
    "# # If you adjusted the notebook display width to be as wide as your screen, the map might get very big. \n",
    "# #Adjust size as desired.\n",
    "# #m.fit_bounds([[lat_range[0], lon_range[0]], [lat_range[1], lon_range[1]]])\n",
    "\n",
    "# folium.CircleMarker((acc_evt_ls['Latitude'], acc_evt_ls['Longitude']), radius=2, \n",
    "#             #color=clust_colors[cluster_id % len(clust_colors)], \n",
    "#             color='red', fill=False, opacity=.3).add_to(m)\n",
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tile = basemap_to_tiles(basemaps.Esri.WorldStreetMap)\n",
    "\n",
    "\n",
    "Esri_WorldStreetMap = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer/tile/{z}/{y}/{x}'\n",
    "Esri_Attribution = 'Tiles &copy; Esri &mdash; Source: Esri, DeLorme, NAVTEQ, USGS, Intermap, iPC, NRCAN, Esri Japan, METI, Esri China (Hong Kong), Esri (Thailand), TomTom, 2012'\n",
    "\n",
    "\n",
    "m = folium.Map(tiles=Esri_WorldStreetMap, attr=Esri_Attribution, width='100%', height='100%', \n",
    "               location=[51.504797 , -0.068721],zoom_start=10) \n",
    "# If you adjusted the notebook display width to be as wide as your screen, the map might get very big. \n",
    "#Adjust size as desired.\n",
    "#m.fit_bounds([[lat_range[0], lon_range[0]], [lat_range[1], lon_range[1]]])\n",
    "for id, row in acc_evt_ls.iterrows():\n",
    "    cluster_id = row[clust_id_col_name]\n",
    "    if cluster_id != -1 and len(np.where(clusters_data.index==cluster_id)[0])>0:\n",
    "        i=np.where(clusters_data.index==cluster_id)[0][0]\n",
    "        if i<len(xy_mds_ST):\n",
    "            folium.CircleMarker((row['Latitude'], row['Longitude']), radius=2, \n",
    "                        #color=clust_colors[cluster_id % len(clust_colors)], \n",
    "                        color=getColor(xy_mds_ST[i,0], xy_mds_ST[i,1],xmin_ST,xmax_ST,ymin_ST,ymax_ST),\n",
    "                        fill=False, opacity=1,\n",
    "                        popup='Cluster: {}'.format(cluster_id)).add_to(m)\n",
    "            \n",
    "\n",
    "#m\n",
    "\n",
    "#m.save(os.path.join('', 'all_accidents_lon.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the map as an HTML file\n",
    "fn='testmap.html'\n",
    "tmpurl='file://{path}/{mapfile}'.format(path=os.getcwd(),mapfile=fn)\n",
    "m.save(fn)\n",
    "\n",
    "#Open a browser window...\n",
    "browser = webdriver.Chrome()\n",
    "#..that displays the map...\n",
    "browser.get(tmpurl)\n",
    "#Give the map tiles some time to load\n",
    "time.sleep(delay)\n",
    "#Grab the screenshot\n",
    "browser.save_screenshot('map_lon_ls_dbscan.png')\n",
    "#Close the browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN Light Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_evt_hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_evt_day = acc_evt_hs[acc_evt_hs.Light_Conditions=='Daylight']\n",
    "acc_evt_drk = acc_evt_hs[acc_evt_hs.Light_Conditions.str.contains('Darkness')]\n",
    "\n",
    "X_day = acc_evt_day[['Longitude','Latitude']].values\n",
    "X_drk = acc_evt_drk[['Longitude','Latitude']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_evt_hs[acc_evt_hs.Light_Conditions.str.contains('Darkness')]['Hour'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN Daylight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df.Light_Conditions=='Daylight']['Hour'][:15].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbors = NearestNeighbors(n_neighbors=7)\n",
    "neighbors = nearest_neighbors.fit(X_day)\n",
    "distances, indices = neighbors.kneighbors(X_day)\n",
    "distances = np.sort(distances[:,6], axis=0)\n",
    "# fig = plt.figure(figsize=(5, 5))\n",
    "# plt.plot(distances)\n",
    "# plt.xlabel(\"Points\")\n",
    "# plt.ylabel(\"Distance\")\n",
    "\n",
    "i = np.arange(len(distances))\n",
    "knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n",
    "# fig = plt.figure(figsize=(5, 5))\n",
    "# knee.plot_knee()\n",
    "# plt.xlabel(\"Points\")\n",
    "# plt.ylabel(\"Distance\")\n",
    "\n",
    "print(distances[knee.knee])\n",
    "#plt.savefig(\"Distance_curve.png\", dpi=300)\n",
    "\n",
    "temporal_dist_max = 24\n",
    "n_neighbours = 7\n",
    "\n",
    "# represent points consistently as (lat, lon)\n",
    "coords = acc_evt_day[['Longitude','Latitude']]\n",
    "\n",
    "\n",
    "# define epsilon as 1.5 kilometers, converted to radians for use by haversine\n",
    "spatial_dist_max = distances[knee.knee] #/ kms_per_radian\n",
    "#spatial_dist_max = 0.00006\n",
    "\n",
    "clustered_ST = DBSCAN(eps=np.radians(distances[knee.knee]),\n",
    "                      metric='haversine',min_samples=n_neighbours).fit(np.radians(coords))\n",
    "\n",
    "print(\"Clustering finished!\")\n",
    "\n",
    "labels=clustered_ST.labels_\n",
    "unique_labels=np.unique(clustered_ST.labels_)\n",
    "print('Result: {} records in the noise, labelled as -1, and {} clusters labelled as 0..{}'.\n",
    "      format(acc_evt_day[labels==-1].shape[0], len(unique_labels)-1, len(unique_labels)-2))\n",
    "\n",
    "#clustered\n",
    "clust_id_col_name='ClusterN'\n",
    "acc_evt_day[clust_id_col_name]=labels\n",
    "## Getting cluster sizes\n",
    "cluster_sizes = acc_evt_day[clust_id_col_name].value_counts().rename_axis('Cluster id').to_frame('count')\n",
    "print(\"Cluster sizes:\")\n",
    "print(cluster_sizes.head(10))\n",
    "\n",
    "cluster_sizes = cluster_sizes[cluster_sizes.index != -1] # no noise\n",
    "\n",
    "max_cluster_size=cluster_sizes['count'].max()\n",
    "print(\"max = \",max_cluster_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_func = {\n",
    "    'DaysSince':['max','min'],\n",
    "    'Longitude':['mean','max','min'],\n",
    "    'Latitude':['mean','max','min']\n",
    "}\n",
    "st_aggregates = acc_evt_day.reset_index(drop=False)[['ClusterN','DaysSince',\n",
    "                                                'Longitude','Latitude']].groupby(['ClusterN']).agg(agg_func)\n",
    "# Flatten hierarchical column names\n",
    "st_aggregates.columns = [\"_\".join(x) for x in st_aggregates.columns.ravel()]\n",
    "# compute derived attributes: duration and bounding rectangle diagonal\n",
    "st_aggregates['duration (days)']=st_aggregates['DaysSince_max']-st_aggregates['DaysSince_min']\n",
    "for id,row in st_aggregates.iterrows():\n",
    "    brd=kms_per_radian*great_circle2(row['Latitude_max'],row['Longitude_max'],row['Latitude_min'],row['Longitude_min'])\n",
    "    #print('{}'.format(brd))\n",
    "    #print(row['Latitude_max'],row['Longitude_max'],row['Latitude_min'],row['Longitude_min'])\n",
    "    st_aggregates.at[id,'Bound_rect_diag(km)']=brd\n",
    "\n",
    "    \n",
    "\n",
    "clusters_data = st_aggregates.loc[st_aggregates.index!=-1,\n",
    "                                  ['Latitude_mean','Longitude_mean',\n",
    "                                   'DaysSince_min','DaysSince_max']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "clusters_data_scaled = scaler.fit_transform(clusters_data)\n",
    "\n",
    "mds_ST = MDS(n_components = 2, random_state=110)\n",
    "mds_ST.fit(clusters_data_scaled)\n",
    "xy_mds_ST = mds_ST.fit_transform(clusters_data_scaled)\n",
    "\n",
    "xmin_ST=xy_mds_ST[:,0].min() \n",
    "xmax_ST=xy_mds_ST[:,0].max()\n",
    "ymin_ST=xy_mds_ST[:,1].min()\n",
    "ymax_ST=xy_mds_ST[:,1].max()\n",
    "#print(xmin_ST,xmax_ST,ymin_ST,ymax_ST)\n",
    "\n",
    "#fig.savefig('mdslonls_dbscan.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = basemap_to_tiles(basemaps.Esri.WorldStreetMap)\n",
    "\n",
    "\n",
    "Esri_WorldStreetMap = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer/tile/{z}/{y}/{x}'\n",
    "Esri_Attribution = 'Tiles &copy; Esri &mdash; Source: Esri, DeLorme, NAVTEQ, USGS, Intermap, iPC, NRCAN, Esri Japan, METI, Esri China (Hong Kong), Esri (Thailand), TomTom, 2012'\n",
    "\n",
    "\n",
    "m = folium.Map(tiles=Esri_WorldStreetMap, attr=Esri_Attribution, width='100%', height='100%', \n",
    "               location=[51.504797 , -0.068721],zoom_start=10) \n",
    "# If you adjusted the notebook display width to be as wide as your screen, the map might get very big. \n",
    "#Adjust size as desired.\n",
    "#m.fit_bounds([[lat_range[0], lon_range[0]], [lat_range[1], lon_range[1]]])\n",
    "for id, row in acc_evt_day.iterrows():\n",
    "    cluster_id = row[clust_id_col_name]\n",
    "    if cluster_id != -1 and len(np.where(clusters_data.index==cluster_id)[0])>0:\n",
    "        i=np.where(clusters_data.index==cluster_id)[0][0]\n",
    "        if i<len(xy_mds_ST):\n",
    "            folium.CircleMarker((row['Latitude'], row['Longitude']), radius=2, \n",
    "                        #color=clust_colors[cluster_id % len(clust_colors)], \n",
    "                        color=getColor(xy_mds_ST[i,0], xy_mds_ST[i,1],xmin_ST,xmax_ST,ymin_ST,ymax_ST),\n",
    "                        fill=False, opacity=1,\n",
    "                        popup='Cluster: {}'.format(cluster_id)).add_to(m)\n",
    "            \n",
    "\n",
    "#m\n",
    "\n",
    "#m.save(os.path.join('', 'all_accidents_lon.html'))\n",
    "#Save the map as an HTML file\n",
    "# fn='testmap.html'\n",
    "# tmpurl='file://{path}/{mapfile}'.format(path=os.getcwd(),mapfile=fn)\n",
    "# m.save(fn)\n",
    "\n",
    "# delay=5\n",
    "# #Open a browser window...\n",
    "# browser = webdriver.Chrome()\n",
    "# #..that displays the map...\n",
    "# browser.get(tmpurl)\n",
    "# #Give the map tiles some time to load\n",
    "# time.sleep(delay)\n",
    "# #Grab the screenshot\n",
    "# browser.save_screenshot('map_lon_day_dbscan.png')\n",
    "# #Close the browser\n",
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN Darkness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbors = NearestNeighbors(n_neighbors=7)\n",
    "neighbors = nearest_neighbors.fit(X_drk)\n",
    "distances, indices = neighbors.kneighbors(X_drk)\n",
    "distances = np.sort(distances[:,6], axis=0)\n",
    "#fig = plt.figure(figsize=(5, 5))\n",
    "#plt.plot(distances)\n",
    "#plt.xlabel(\"Points\")\n",
    "#plt.ylabel(\"Distance\")\n",
    "\n",
    "i = np.arange(len(distances))\n",
    "knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n",
    "#fig = plt.figure(figsize=(5, 5))\n",
    "#knee.plot_knee()\n",
    "#plt.xlabel(\"Points\")\n",
    "#plt.ylabel(\"Distance\")\n",
    "\n",
    "print(distances[knee.knee])\n",
    "#plt.savefig(\"Distance_curve.png\", dpi=300)\n",
    "\n",
    "temporal_dist_max = 24\n",
    "n_neighbours = 7\n",
    "\n",
    "# represent points consistently as (lat, lon)\n",
    "coords = acc_evt_drk[['Longitude','Latitude']]\n",
    "\n",
    "\n",
    "# define epsilon as 1.5 kilometers, converted to radians for use by haversine\n",
    "spatial_dist_max = distances[knee.knee] #/ kms_per_radian\n",
    "#spatial_dist_max = 0.00006\n",
    "\n",
    "clustered_ST = DBSCAN(eps=np.radians(distances[knee.knee]),\n",
    "                      metric='haversine',min_samples=n_neighbours).fit(np.radians(coords))\n",
    "\n",
    "print(\"Clustering finished!\")\n",
    "\n",
    "labels=clustered_ST.labels_\n",
    "unique_labels=np.unique(clustered_ST.labels_)\n",
    "print('Result: {} records in the noise, labelled as -1, and {} clusters labelled as 0..{}'.\n",
    "      format(acc_evt_drk[labels==-1].shape[0], len(unique_labels)-1, len(unique_labels)-2))\n",
    "\n",
    "#clustered\n",
    "clust_id_col_name='ClusterN'\n",
    "acc_evt_drk[clust_id_col_name]=labels\n",
    "## Getting cluster sizes\n",
    "cluster_sizes = acc_evt_drk[clust_id_col_name].value_counts().rename_axis('Cluster id').to_frame('count')\n",
    "print(\"Cluster sizes:\")\n",
    "print(cluster_sizes.head(10))\n",
    "\n",
    "cluster_sizes = cluster_sizes[cluster_sizes.index != -1] # no noise\n",
    "\n",
    "max_cluster_size=cluster_sizes['count'].max()\n",
    "print(\"max = \",max_cluster_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_func = {\n",
    "    'DaysSince':['max','min'],\n",
    "    'Longitude':['mean','max','min'],\n",
    "    'Latitude':['mean','max','min']\n",
    "}\n",
    "st_aggregates = acc_evt_drk.reset_index(drop=False)[['ClusterN','DaysSince',\n",
    "                                                'Longitude','Latitude']].groupby(['ClusterN']).agg(agg_func)\n",
    "# Flatten hierarchical column names\n",
    "st_aggregates.columns = [\"_\".join(x) for x in st_aggregates.columns.ravel()]\n",
    "# compute derived attributes: duration and bounding rectangle diagonal\n",
    "st_aggregates['duration (days)']=st_aggregates['DaysSince_max']-st_aggregates['DaysSince_min']\n",
    "for id,row in st_aggregates.iterrows():\n",
    "    brd=kms_per_radian*great_circle2(row['Latitude_max'],row['Longitude_max'],row['Latitude_min'],row['Longitude_min'])\n",
    "    #print('{}'.format(brd))\n",
    "    #print(row['Latitude_max'],row['Longitude_max'],row['Latitude_min'],row['Longitude_min'])\n",
    "    st_aggregates.at[id,'Bound_rect_diag(km)']=brd\n",
    "\n",
    "    \n",
    "\n",
    "clusters_data = st_aggregates.loc[st_aggregates.index!=-1,\n",
    "                                  ['Latitude_mean','Longitude_mean',\n",
    "                                   'DaysSince_min','DaysSince_max']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "clusters_data_scaled = scaler.fit_transform(clusters_data)\n",
    "\n",
    "mds_ST = MDS(n_components = 2, random_state=110)\n",
    "mds_ST.fit(clusters_data_scaled)\n",
    "xy_mds_ST = mds_ST.fit_transform(clusters_data_scaled)\n",
    "\n",
    "xmin_ST=xy_mds_ST[:,0].min() \n",
    "xmax_ST=xy_mds_ST[:,0].max()\n",
    "ymin_ST=xy_mds_ST[:,1].min()\n",
    "ymax_ST=xy_mds_ST[:,1].max()\n",
    "#print(xmin_ST,xmax_ST,ymin_ST,ymax_ST)\n",
    "\n",
    "#fig.savefig('mdslonls_dbscan.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = basemap_to_tiles(basemaps.Esri.WorldStreetMap)\n",
    "\n",
    "\n",
    "Esri_WorldStreetMap = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer/tile/{z}/{y}/{x}'\n",
    "Esri_Attribution = 'Tiles &copy; Esri &mdash; Source: Esri, DeLorme, NAVTEQ, USGS, Intermap, iPC, NRCAN, Esri Japan, METI, Esri China (Hong Kong), Esri (Thailand), TomTom, 2012'\n",
    "\n",
    "\n",
    "m = folium.Map(tiles=Esri_WorldStreetMap, attr=Esri_Attribution, width='100%', height='100%', \n",
    "               location=[51.504797 , -0.068721],zoom_start=10) \n",
    "# If you adjusted the notebook display width to be as wide as your screen, the map might get very big. \n",
    "#Adjust size as desired.\n",
    "#m.fit_bounds([[lat_range[0], lon_range[0]], [lat_range[1], lon_range[1]]])\n",
    "for id, row in acc_evt_drk.iterrows():\n",
    "    cluster_id = row[clust_id_col_name]\n",
    "    if cluster_id != -1 and len(np.where(clusters_data.index==cluster_id)[0])>0:\n",
    "        i=np.where(clusters_data.index==cluster_id)[0][0]\n",
    "        if i<len(xy_mds_ST):\n",
    "            folium.CircleMarker((row['Latitude'], row['Longitude']), radius=2, \n",
    "                        #color=clust_colors[cluster_id % len(clust_colors)], \n",
    "                        color=getColor(xy_mds_ST[i,0], xy_mds_ST[i,1],xmin_ST,xmax_ST,ymin_ST,ymax_ST),\n",
    "                        fill=False, opacity=1,\n",
    "                        popup='Cluster: {}'.format(cluster_id)).add_to(m)\n",
    "            \n",
    "\n",
    "m\n",
    "\n",
    "# #m.save(os.path.join('', 'all_accidents_lon.html'))\n",
    "# #Save the map as an HTML file\n",
    "# fn='testmap.html'\n",
    "# tmpurl='file://{path}/{mapfile}'.format(path=os.getcwd(),mapfile=fn)\n",
    "# m.save(fn)\n",
    "\n",
    "# delay=5\n",
    "# #Open a browser window...\n",
    "# browser = webdriver.Chrome()\n",
    "# #..that displays the map...\n",
    "# browser.get(tmpurl)\n",
    "# #Give the map tiles some time to load\n",
    "# time.sleep(delay)\n",
    "# #Grab the screenshot\n",
    "# browser.save_screenshot('map_lon_drk_dbscan.png')\n",
    "# #Close the browser\n",
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "X = load_iris().data\n",
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg1_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_evt_hdb_hs = acc_evt_hs.copy()\n",
    "acc_evt_hdb_ls = acc_evt_ls.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDBSCAN\n",
    "#import kdbscan\n",
    "import importlib\n",
    "importlib.reload(kdbscan)\n",
    "from kdbscan import KDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_evt_hdb_hs[acc_evt_hdb_hs.Year==2017].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg1 = KDBSCAN(h=0.1,t=0.18,metric=cosine)\n",
    "\n",
    "#X = load_iris().data\n",
    "\n",
    "kde = alg1.fit(acc_evt_hdb_hs[acc_evt_hdb_hs.Year==2017][['Latitude','Longitude']].values,return_kde = True)\n",
    "#kde = alg1.fit(X[:,1:3],return_kde = True)\n",
    "\n",
    "alg1_labels = alg1.labels_\n",
    "c1, c2, kept = kdbscan.plot_kdbscan_results(kde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg1 = KDBSCAN(h=0.1,t=0.18,metric=cosine)\n",
    "\n",
    "#X = load_iris().data\n",
    "\n",
    "kde = alg1.fit(acc_evt_hdb_hs[acc_evt_hdb_hs.Year==2017][['Latitude','Longitude']].values,return_kde = True)\n",
    "#kde = alg1.fit(X[:,1:3],return_kde = True)\n",
    "\n",
    "alg1_labels = alg1.labels_\n",
    "c1, c2, kept = kdbscan.plot_kdbscan_results(kde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, c2, kept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_boundaries = gb_boundaries[(gb_boundaries.Year.isin(['2017']))]\n",
    "#&                                     (gb_boundaries.Region.isin(['Scotland','East']))]\n",
    "bound_val = filtered_boundaries[(filtered_boundaries.Region=='London')\n",
    "                               #& (filtered_boundaries.geo_code=='E09000021')\n",
    "                               ].reset_index()\n",
    "bound_val[bound_val.lad20nm.str.contains('Lond')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "url='https://github.com/martinjc/UK-GeoJSON/blob/master/json/administrative/eng/lad.json?raw=true'\n",
    "r = requests.get(url)\n",
    "\n",
    "with open(\"lad.json\", \"wb\") as code:\n",
    "    code.write(r.content)\n",
    "\n",
    "#Free up memory...\n",
    "r=None\n",
    "\n",
    "with open('lad.json', 'r') as output:\n",
    "    boundaries = json.load(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_val.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_boroughs = ['Westminster',\n",
    "'Haringey',\n",
    "'Ealing',\n",
    "'Croydon',\n",
    "'Barnet',\n",
    "'Barking and Dagenham',\n",
    "'Hounslow',\n",
    "'Kingston upon Thames',\n",
    "'Barnet',\n",
    "'Greenwich',\n",
    "'Havering',\n",
    "'Bexley',\n",
    "'Redbridge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = basemap_to_tiles(basemaps.Esri.WorldStreetMap)\n",
    "\n",
    "\n",
    "Esri_WorldStreetMap = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer/tile/{z}/{y}/{x}'\n",
    "Esri_Attribution = 'Tiles &copy; Esri &mdash; Source: Esri, DeLorme, NAVTEQ, USGS, Intermap, iPC, NRCAN, Esri Japan, METI, Esri China (Hong Kong), Esri (Thailand), TomTom, 2012'\n",
    "\n",
    "#f = folium.Figure(width=1510, height=810)\n",
    "m = folium.Map(tiles=white_tile,attr='white tile', \n",
    "               width='100%', height='100%',\n",
    "               location=[51.504797 , -0.068721],zoom_start=10)\n",
    "\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=boundaries,\n",
    "    data=bound_val,\n",
    "    columns=['geo_code', 'High_Severity'],\n",
    "    key_on='feature.properties.LAD13CD',\n",
    "    fill_color='YlGnBu',\n",
    "    fill_opacity=0.8,\n",
    "    line_opacity=0.4,\n",
    "    nan_fill_color='white',\n",
    "    nan_fill_opacity=1,\n",
    "    legend_name='High severity Accidents in 2017',\n",
    "    \n",
    ").add_to(m)\n",
    "\n",
    "vb_idx=0\n",
    "for idx in range(c1.shape[0]):\n",
    "    if kept[idx]:\n",
    "        folium.CircleMarker((c2[idx], c1[idx]), radius=4, \n",
    "                    color='magenta',\n",
    "                    fill=False, opacity=1).add_to(m)\n",
    "        if valid_boroughs[vb_idx] == 'Greenwich':\n",
    "            folium.map.Marker(\n",
    "                        (c2[idx], c1[idx]),\n",
    "                        icon=DivIcon(\n",
    "                            icon_size=(150,36),\n",
    "                            icon_anchor=(0,0),\n",
    "                            html='<div style=\"left: -50px; position: relative; font-size: 12pt\"><b>%s</b></div>' % str(valid_boroughs[vb_idx]),\n",
    "                            )\n",
    "                        ).add_to(m)\n",
    "        else:\n",
    "            folium.map.Marker(\n",
    "                        (c2[idx], c1[idx]),\n",
    "                        icon=DivIcon(\n",
    "                            icon_size=(150,36),\n",
    "                            icon_anchor=(0,0),\n",
    "                            html='<div style=\"font-size: 12pt\"><b>%s</b></div>' % str(valid_boroughs[vb_idx]),\n",
    "                            )\n",
    "                        ).add_to(m)\n",
    "        vb_idx+=1\n",
    "\n",
    "        \n",
    "folium.TileLayer('openstreetmap').add_to(m)\n",
    "\n",
    "#m\n",
    "\n",
    "\n",
    "\n",
    "fn='testmap.html'\n",
    "tmpurl='file://{path}/{mapfile}'.format(path=os.getcwd(),mapfile=fn)\n",
    "m.save(fn)\n",
    "\n",
    "delay=3\n",
    "#Open a browser window...\n",
    "browser = webdriver.Chrome()\n",
    "#..that displays the map...\n",
    "browser.get(tmpurl)\n",
    "#Give the map tiles some time to load\n",
    "time.sleep(delay)\n",
    "#Grab the screenshot\n",
    "browser.save_screenshot('map_lon_hdbscan.png')\n",
    "#Close the browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meanshift implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# define the model\n",
    "ms_model = MeanShift()\n",
    "# fit model and predict clusters\n",
    "yhat = ms_model.fit_predict(X_hs)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "    # get row indexes for samples with this cluster\n",
    "    row_ix = where(yhat == cluster)\n",
    "    # create scatter of these samples\n",
    "    plt.scatter(X_hs[row_ix, 0], X_hs[row_ix, 1])\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(centroids.shape[0]):\n",
    "#     if kept[idx]:\n",
    "#         ax.plot(c1[idx], c2[idx], 'm^', markersize=20)\n",
    "#     else:\n",
    "#         ax.plot(c1[idx], c2[idx], 'k^', markersize=18)  \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ran = df[((df.Region == 'London') | (df.Region == 'Wales')) &\n",
    "             (df.Accident_Severity !='Slight') &\n",
    "             (df.Year >= 2017)][['Longitude','Latitude']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = basemap_to_tiles(basemaps.Esri.WorldStreetMap)\n",
    "\n",
    "\n",
    "Esri_WorldStreetMap = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer/tile/{z}/{y}/{x}'\n",
    "Esri_Attribution = 'Tiles &copy; Esri &mdash; Source: Esri, DeLorme, NAVTEQ, USGS, Intermap, iPC, NRCAN, Esri Japan, METI, Esri China (Hong Kong), Esri (Thailand), TomTom, 2012'\n",
    "\n",
    "\n",
    "m = folium.Map(tiles=Esri_WorldStreetMap, attr=Esri_Attribution, width='100%', height='100%', \n",
    "               location=[51.504797 , -0.068721],zoom_start=10) \n",
    "# If you adjusted the notebook display width to be as wide as your screen, the map might get very big. \n",
    "#Adjust size as desired.\n",
    "#m.fit_bounds([[lat_range[0], lon_range[0]], [lat_range[1], lon_range[1]]])\n",
    "for id, row in acc_evt_hs.iterrows():\n",
    "    cluster_id = row[clust_id_col_name]\n",
    "    if cluster_id != -1 and len(np.where(clusters_data.index==cluster_id)[0])>0:\n",
    "        i=np.where(clusters_data.index==cluster_id)[0][0]\n",
    "        if i<len(xy_mds_ST):\n",
    "            folium.CircleMarker((row['Latitude'], row['Longitude']), radius=2, \n",
    "                        #color=clust_colors[cluster_id % len(clust_colors)], \n",
    "                        color=getColor(xy_mds_ST[i,0], xy_mds_ST[i,1],xmin_ST,xmax_ST,ymin_ST,ymax_ST),\n",
    "                        fill=False, opacity=0.7,\n",
    "                        popup='Cluster: {}'.format(cluster_id)).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial-temp DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring spatial variation in the Number of accidents per LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Region=='London'][['Local_Authority_(District)','Area_Code']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a2013 = df_group_altair(df,2013)\n",
    "df_a2014 = df_group_altair(df,2014)\n",
    "df_a2015 = df_group_altair(df,2015)\n",
    "df_a2016 = df_group_altair(df,2016)\n",
    "df_a2017 = df_group_altair(df,2017)\n",
    "\n",
    "df_alt = pd.concat([df_a2013, \n",
    "                           df_a2014, \n",
    "                           df_a2015, \n",
    "                           df_a2016, \n",
    "                           df_a2017], axis=0)\n",
    "df_alt = df_alt.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# uk_region = uk_region.append({'Area_Code': 'E09000001', 'Region': 'London', 'Area': 'City of London'}, ignore_index=True)\n",
    "# a = uk_region[['Area_Code','Region','Area']]\n",
    "# a[a.Area_Code.str.contains('E0900000')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpaceTimeDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gb_boundaries = pd.merge(gb,referendum_data,left_on='geo_code', right_on='Area_Code', how='inner')\n",
    "gb_boundaries = pd.merge(gb,df_alt,left_on='geo_code', right_on='Area_Code', how='inner')\n",
    "gb_boundaries = pd.merge(gb_boundaries,uk_region[['Area_Code','Region','Area']],left_on='geo_code', \n",
    "                         right_on='Area_Code', how='inner')\n",
    "\n",
    "gb_boundaries.rename(columns={('Accident_Index', 'Fatal'): 'Accidents_Fatal'}, inplace=True)\n",
    "gb_boundaries.rename(columns={('Accident_Index', 'Serious'): 'Accidents_Serious'}, inplace=True)\n",
    "gb_boundaries.rename(columns={('Accident_Index', 'Slight'): 'Accidents_Slight'}, inplace=True)\n",
    "gb_boundaries.rename(columns={('Casualties', ''): 'Casualties'}, inplace=True)\n",
    "gb_boundaries.rename(columns={('Year', ''): 'Year'}, inplace=True)\n",
    "gb_boundaries['High_Severity'] = gb_boundaries['Accidents_Serious'] + gb_boundaries['Accidents_Fatal']\n",
    "gb_boundaries['Year'] = gb_boundaries['Year'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "altCentroid = pd.DataFrame({'Longitude': c1, 'Latitude': c2, 'Valid': kept})\n",
    "altCentroid[altCentroid.Valid==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_boundaries = gb_boundaries[(gb_boundaries.Year.isin(['2017']))]\n",
    "#&                                     (gb_boundaries.Region.isin(['Scotland','East']))]\n",
    "\n",
    "for idx in range(c1.shape[0]):\n",
    "    if kept[idx]:\n",
    "        folium.CircleMarker((c2[idx], c1[idx]), radius=4, \n",
    "                    #color=clust_colors[cluster_id % len(clust_colors)], \n",
    "                    color='black',\n",
    "                    fill=False, opacity=1).add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_geo = alt.InlineData(values = filtered_boundaries\n",
    "                          [filtered_boundaries.Region=='London'].to_json(), #geopandas to geojson string\n",
    "                       format = alt.DataFormat(property='features',type='json'))\n",
    "\n",
    "base = alt.Chart(data_geo).properties(\n",
    "    projection={'type': 'identity','reflectY': True},\n",
    "    width=500,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# chart13 = base.mark_geoshape(strokeWidth=1,stroke='lightgray',strokeOpacity=0.2\n",
    "# ).encode(\n",
    "#     color=alt.Color('properties.Accidents_Fatal:Q', scale=alt.Scale(scheme='blueorange')),\n",
    "#     tooltip=['properties.Area:N','properties.Accidents_Fatal:Q','properties.Region:N'],\n",
    "# ).transform_filter(\n",
    "#     ('properties.Area:N' == 2013)\n",
    "# )\n",
    "\n",
    "chartpoints = alt.Chart(altCentroid[altCentroid.Valid==True]).mark_circle().encode(\n",
    "    longitude='Longitude:Q',\n",
    "    latitude='Latitude:Q',\n",
    "    size=alt.value(10)\n",
    ")\n",
    "\n",
    "chart17 = base.mark_geoshape(strokeWidth=1,stroke='lightgray',strokeOpacity=0.2\n",
    ").encode(\n",
    "    color=alt.Color('properties.High_Severity:Q', scale=alt.Scale(scheme='blueorange')),\n",
    "    tooltip=['properties.Area:N','properties.High_Severity:Q','properties.Region:N'],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chart17 + chartpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_geo = alt.InlineData(values = filtered_boundaries[filtered_boundaries['Region'] == 'London'].to_json(), #geopandas to geojson string\n",
    "                       format = alt.DataFormat(property='features',type='json'))\n",
    "\n",
    "alt.Chart(data_geo).mark_geoshape(strokeWidth=1,stroke='lightgray',strokeOpacity=0.2\n",
    ").encode(\n",
    "    color=alt.Color('properties.High_Severity:Q', scale=alt.Scale(scheme='blueorange')),\n",
    "    tooltip=['properties.Area:N','properties.Casualties:Q','properties.Region:N']\n",
    ").properties(\n",
    "    projection={'type': 'identity','reflectY': True},\n",
    "    width=500,\n",
    "    height=600\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = basemap_to_tiles(basemaps.Esri.WorldStreetMap)\n",
    "\n",
    "\n",
    "Esri_WorldStreetMap = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer/tile/{z}/{y}/{x}'\n",
    "Esri_Attribution = 'Tiles &copy; Esri &mdash; Source: Esri, DeLorme, NAVTEQ, USGS, Intermap, iPC, NRCAN, Esri Japan, METI, Esri China (Hong Kong), Esri (Thailand), TomTom, 2012'\n",
    "\n",
    "# Create a white image of 4 pixels, and embed it in a url.\n",
    "white_tile = branca.utilities.image_to_url([[1, 1], [1, 1]])\n",
    "\n",
    "m = folium.Map(tiles=white_tile,attr='white tile',\n",
    "               dragging=True, width='100%', height='100%', \n",
    "               location=[51.504797 , -0.068721],zoom_start=10) \n",
    "# If you adjusted the notebook display width to be as wide as your screen, the map might get very big. \n",
    "#Adjust size as desired.\n",
    "\n",
    "g = folium.Choropleth(\n",
    "    geo_data=gb_boundaries,\n",
    "    data=gb,\n",
    "    columns=['High_Severity'],\n",
    "    key_on='properties.High_Severity',\n",
    "    fill_color='YlGn',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.4,\n",
    "    legend_name='Data Points',\n",
    "    highlight=True,\n",
    ").add_to(m)\n",
    "\n",
    "# for idx in range(c1.shape[0]):\n",
    "#     if kept[idx]:\n",
    "#         folium.CircleMarker((c2[idx], c1[idx]), radius=4, \n",
    "#                     #color=clust_colors[cluster_id % len(clust_colors)], \n",
    "#                     color='black',\n",
    "#                     fill=False, opacity=1).add_to(m)\n",
    "\n",
    "\n",
    "m\n",
    "\n",
    "#m.save(os.path.join('', 'all_accidents_lon.html'))\n",
    "#Save the map as an HTML file\n",
    "# fn='testmap.html'\n",
    "# tmpurl='file://{path}/{mapfile}'.format(path=os.getcwd(),mapfile=fn)\n",
    "# m.save(fn)\n",
    "\n",
    "# delay=5\n",
    "# #Open a browser window...\n",
    "# browser = webdriver.Chrome()\n",
    "# #..that displays the map...\n",
    "# browser.get(tmpurl)\n",
    "# #Give the map tiles some time to load\n",
    "# time.sleep(delay)\n",
    "# #Grab the screenshot\n",
    "# browser.save_screenshot('map_lon_drk_dbscan.png')\n",
    "# #Close the browser\n",
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather distribution per Year (London)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = acc_lon.groupby(['Year','Weather_Conditions']).count()\n",
    "df_weather = df_weather.sort_values(['Accident_Index'],ascending=False).reset_index()\n",
    "df_weather.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot barh chart with index as x values\n",
    "\n",
    "g = sns.catplot(x='Weather_Conditions', y='Accident_Index', \n",
    "            col=\"Year\", col_wrap=5,\n",
    "            kind='bar', data=df_weather, height=18, aspect= 0.5)\n",
    "\n",
    "g.set_xticklabels(rotation=90,size = 30)\n",
    "g.set_yticklabels(size = 100)\n",
    "g.set_titles(size = 32)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = acc_evt[[\"Latitude\",\"Longitude\"]]\n",
    "map_shape = (4,4)\n",
    "## scale data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_preprocessed = scaler.fit_transform(X.values)\n",
    "## clustering\n",
    "model = minisom.MiniSom(x=map_shape[0], y=map_shape[1], \n",
    "                        input_len=X.shape[1])\n",
    "model.train_batch(X_preprocessed, num_iteration=100, verbose=False)\n",
    "## build output dataframe\n",
    "dtf_X = X.copy()\n",
    "dtf_X[\"cluster\"] = np.ravel_multi_index(np.array(\n",
    "      [model.winner(x) for x in X_preprocessed]).T, dims=map_shape)\n",
    "## find real centroids\n",
    "#cluster_centers = np.array([vec for center in model.get_weights() \n",
    "#                            for vec in center])\n",
    "\n",
    "#cluster_centers, _ = kmeans(X_preprocessed, 5) \n",
    "\n",
    "closest, distances = scipy.cluster.vq.vq(cluster_centers, X_preprocessed)\n",
    "\n",
    "dtf_X[\"centroids\"] = 0\n",
    "\n",
    "for i in closest:\n",
    "    dtf_X[\"centroids\"].iloc[i] = 1\n",
    "## add clustering info to the original dataset\n",
    "acc_evt[[\"cluster\",\"centroids\"]] = dtf_X[[\"cluster\",\"centroids\"]]\n",
    "## plot\n",
    "k = acc_evt[\"cluster\"].nunique()\n",
    "fig, ax = plt.subplots(figsize=(14,8))\n",
    "sns.scatterplot(x=\"Latitude\", y=\"Longitude\", data=acc_evt, \n",
    "                palette=sns.color_palette(\"bright\",k),\n",
    "                hue='cluster', size=\"centroids\", size_order=[1,0],\n",
    "                legend=\"brief\", ax=ax).set_title('Clustering (k='+str(k)+')')\n",
    "th_centroids = scaler.inverse_transform(cluster_centers)\n",
    "ax.scatter(th_centroids[:,0], th_centroids[:,1], s=50, c='black', \n",
    "           marker=\"x\")\n",
    "\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_boundaries.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Declaring variables for use\n",
    "distortions = []\n",
    "# Populating distortions for various clusters\n",
    "\n",
    "num_clusters = range(2, 15)\n",
    "\n",
    "for i in num_clusters:\n",
    "    centroids, distortion = kmeans(X_preprocessed, i)\n",
    "    distortions.append(distortion)\n",
    "    \n",
    "# Plotting elbow plot data\n",
    "elbow_plot_data = pd.DataFrame({'num_clusters': num_clusters,\n",
    "'distortions': distortions})\n",
    "sns.lineplot(x='num_clusters', y='distortions',\n",
    "data = elbow_plot_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = \"Latitude\", \"Longitude\"\n",
    "color = \"cluster\"\n",
    "size = \"Number_of_Casualties\"\n",
    "popup1 = \"Number_of_Casualties\"\n",
    "popup2 = \"Number_of_Vehicles\"\n",
    "marker = \"centroids\"\n",
    "data = acc_evt.copy()\n",
    "## create color column\n",
    "lst_elements = sorted(list(acc_evt[color].unique()))\n",
    "lst_colors = ['#%06X' % np.random.randint(0, 0xFFFFFF) for i in \n",
    "              range(len(lst_elements))]\n",
    "data[\"color\"] = data[color].apply(lambda x: \n",
    "                lst_colors[lst_elements.index(x)])\n",
    "## create size column (scaled)\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(3,15))\n",
    "data[\"size\"] = scaler.fit_transform(\n",
    "               data[size].values.reshape(-1,1)).reshape(-1)\n",
    "## initialize the map with the starting location\n",
    "map_ = folium.Map(location=[51.584797 , 0.138721], tiles=\"cartodbpositron\",\n",
    "                  zoom_start=9.8)\n",
    "## add points\n",
    "data.apply(lambda row: folium.CircleMarker(\n",
    "           location=[row[x],row[y]], popup='Number of Casualties:' + str(row[popup1]) + '<br>' + \n",
    "           'Number of Vehicles:' + str(row[popup2]),\n",
    "           color=row[\"color\"], fill=True,\n",
    "           radius=row[\"size\"]).add_to(map_), axis=1)\n",
    "## add html legend\n",
    "legend_html = \"\"\"<div style=\"position:fixed; bottom:10px; left:10px; border:2px solid black; z-index:9999; font-size:14px;\">&nbsp;<b>\"\"\"+color+\"\"\":</b><br>\"\"\"\n",
    "for i in lst_elements:\n",
    "     legend_html = legend_html+\"\"\"&nbsp;<i class=\"fa fa-circle \n",
    "     fa-1x\" style=\"color:\"\"\"+lst_colors[lst_elements.index(i)]+\"\"\"\">\n",
    "     </i>&nbsp;\"\"\"+str(i)+\"\"\"<br>\"\"\"\n",
    "legend_html = legend_html+\"\"\"</div>\"\"\"\n",
    "map_.get_root().html.add_child(folium.Element(legend_html))\n",
    "## add centroids marker\n",
    "lst_elements = sorted(list(acc_evt[marker].unique()))\n",
    "# data[data[marker]==1].apply(lambda row: \n",
    "#            folium.Marker(location=[row[x],row[y]], \n",
    "#            popup=row[marker], draggable=False,          \n",
    "#            icon=folium.Icon(color=\"black\")).add_to(map_), axis=1)\n",
    "## plot the map\n",
    "map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssd = []\n",
    "for i in range(2, 26):\n",
    "    km = MiniBatchKMeans(n_clusters=i)\n",
    "    km.fit_predict(X)\n",
    "    ssd.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple casualties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = acc_lon[acc_lon.Number_of_Casualties > 1][[\"Latitude\",\"Longitude\"]]\n",
    "map_shape = (4,4)\n",
    "## scale data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_preprocessed = scaler.fit_transform(X.values)\n",
    "## clustering\n",
    "model = minisom.MiniSom(x=map_shape[0], y=map_shape[1], \n",
    "                        input_len=X.shape[1])\n",
    "model.train_batch(X_preprocessed, num_iteration=100, verbose=False)\n",
    "## build output dataframe\n",
    "dtf_X = X.copy()\n",
    "dtf_X[\"cluster\"] = np.ravel_multi_index(np.array(\n",
    "      [model.winner(x) for x in X_preprocessed]).T, dims=map_shape)\n",
    "## find real centroids\n",
    "cluster_centers = np.array([vec for center in model.get_weights() \n",
    "                            for vec in center])\n",
    "\n",
    "\n",
    "closest, distances = scipy.cluster.vq.vq(cluster_centers, X_preprocessed)\n",
    "\n",
    "dtf_X[\"centroids\"] = 0\n",
    "for i in closest:\n",
    "    dtf_X[\"centroids\"].iloc[i] = 1\n",
    "## add clustering info to the original dataset\n",
    "acc_lon[[\"cluster\",\"centroids\"]] = dtf_X[[\"cluster\",\"centroids\"]]\n",
    "## plot\n",
    "k = acc_lon[\"cluster\"].nunique()\n",
    "\n",
    "lst_elements = sorted(list(acc_lon[marker].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = \"Latitude\", \"Longitude\"\n",
    "color = \"cluster\"\n",
    "size = \"Number_of_Casualties\"\n",
    "popup1 = \"Number_of_Casualties\"\n",
    "popup2 = \"Number_of_Vehicles\"\n",
    "marker = \"centroids\"\n",
    "data = acc_lon[acc_lon.Number_of_Casualties > 1].copy()\n",
    "## create color column\n",
    "lst_elements = sorted(list(acc_lon[color].unique()))\n",
    "lst_colors = ['#%06X' % np.random.randint(0, 0xFFFFFF) for i in \n",
    "              range(len(lst_elements))]\n",
    "data[\"color\"] = data[color].apply(lambda x: \n",
    "                lst_colors[lst_elements.index(x)])\n",
    "## create size column (scaled)\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(3,15))\n",
    "data[\"size\"] = scaler.fit_transform(\n",
    "               data[size].values.reshape(-1,1)).reshape(-1)\n",
    "## initialize the map with the starting location\n",
    "map_ = folium.Map(location=[51.584797 , 0.138721], tiles=\"cartodbpositron\",\n",
    "                  zoom_start=9.8)\n",
    "## add points\n",
    "data.apply(lambda row: folium.CircleMarker(\n",
    "           location=[row[x],row[y]], popup='Number of Casualties:' + str(row[popup1]) + '<br>' + \n",
    "           'Number of Vehicles:' + str(row[popup2]),\n",
    "           color=row[\"color\"], fill=True,\n",
    "           radius=row[\"size\"]).add_to(map_), axis=1)\n",
    "## add html legend\n",
    "legend_html = \"\"\"<div style=\"position:fixed; bottom:10px; left:10px; border:2px solid black; z-index:9999; font-size:14px;\">&nbsp;<b>\"\"\"+color+\"\"\":</b><br>\"\"\"\n",
    "for i in lst_elements:\n",
    "     legend_html = legend_html+\"\"\"&nbsp;<i class=\"fa fa-circle \n",
    "     fa-1x\" style=\"color:\"\"\"+lst_colors[lst_elements.index(i)]+\"\"\"\">\n",
    "     </i>&nbsp;\"\"\"+str(i)+\"\"\"<br>\"\"\"\n",
    "legend_html = legend_html+\"\"\"</div>\"\"\"\n",
    "map_.get_root().html.add_child(folium.Element(legend_html))\n",
    "## add centroids marker\n",
    "lst_elements = sorted(list(acc_lon[marker].unique()))\n",
    "\n",
    "## plot the map\n",
    "map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptually uniform colour space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# first draw a circle in the cylindrical JCh color space. \n",
    "# the third channel is hue in degrees. First is lightness and the second chroma\n",
    "color_circle = np.ones((256,3))*60\n",
    "color_circle[:,1] = np.ones((256))*45\n",
    "color_circle[:,2] = np.arange(0,360,360/256)\n",
    "color_circle_rgb = cspace_convert(color_circle, \"JCh\",\"sRGB1\")\n",
    "\n",
    "cm = col.ListedColormap(color_circle_rgb)\n",
    "\n",
    "##### generate data grid like in above\n",
    "N=256\n",
    "x = np.linspace(-1,1,N)\n",
    "y = np.linspace(-1,1,N)\n",
    "z = np.zeros((len(y),len(x))) # make cartesian grid\n",
    "for ii in range(len(y)): \n",
    "    z[ii] = np.arctan2(y[ii],x) # simple angular function\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax = plt.gca()\n",
    "pmesh = ax.pcolormesh(x, y, (z/np.pi)*1, \n",
    "    cmap = cm, vmin=-1, vmax=1)\n",
    "plt.axis([x.min(), x.max(), y.min(), y.max()])\n",
    "cbar = fig.colorbar(pmesh)\n",
    "cbar.ax.set_ylabel('Phase [pi]')\n",
    "\n",
    "print(np.arctan2(0.5,0)/np.pi)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_points_per_cluster = 250\n",
    "\n",
    "\n",
    "clust = OPTICS(min_samples=50, xi=.25, min_cluster_size=.05)\n",
    "\n",
    "# Run the fit\n",
    "clust.fit(X_hsall)\n",
    "\n",
    "space = np.arange(len(X_hsall))\n",
    "reachability = clust.reachability_[clust.ordering_]\n",
    "labels = clust.labels_[clust.ordering_]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "G = gridspec.GridSpec(2, 3)\n",
    "ax1 = plt.subplot(G[0, :])\n",
    "ax2 = plt.subplot(G[1, 0])\n",
    "\n",
    "# Reachability plot\n",
    "colors = ['g.', 'r.', 'b.', 'y.', 'c.']\n",
    "for klass, color in zip(range(0, 5), colors):\n",
    "    Xk = space[labels == klass]\n",
    "    Rk = reachability[labels == klass]\n",
    "    ax1.plot(Xk, Rk, color, alpha=0.3)\n",
    "ax1.plot(space[labels == -1], reachability[labels == -1], 'k.', alpha=0.3)\n",
    "#ax1.plot(space, np.full_like(space, 2., dtype=float), 'k-', alpha=0.5)\n",
    "#ax1.plot(space, np.full_like(space, 0.5, dtype=float), 'k-.', alpha=0.5)\n",
    "ax1.set_ylabel('Reachability (epsilon distance)')\n",
    "ax1.set_title('Reachability Plot')\n",
    "\n",
    "# OPTICS\n",
    "colors = ['g.', 'r.', 'b.', 'y.', 'c.']\n",
    "for klass, color in zip(range(0, 5), colors):\n",
    "    Xk = X_hsall[clust.labels_ == klass]\n",
    "    ax2.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)\n",
    "\n",
    "ax2.plot(X_hsall[clust.labels_ == -1, 0], X_hsall[clust.labels_ == -1, 1], 'k+', alpha=0.1)\n",
    "\n",
    "ax2.set_title('Automatic Clustering\\nOPTICS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
